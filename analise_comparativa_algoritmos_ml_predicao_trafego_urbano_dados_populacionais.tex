\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Análise Comparativa de Algoritmos de Aprendizado de Máquina na Predição do Crescimento do Trânsito Urbano a partir de Dados Populacionais\\
}

\author{\IEEEauthorblockN{Alexandre Marques Tortoza Canoa}
\IEEEauthorblockA{\textit{Escola Politécnica} \\
\textit{Pontifícia Universidade Católica do Paraná (PUCPR))}\\
Curitiba, PR, Brasil \\
a.marquestortoza@gmail.com}
}


\maketitle

\begin{abstract}
O agravamento do congestionamento urbano, impulsionado pelo crescimento populacional e pela expansão da frota de veículos, afeta diretamente a economia, o meio ambiente e o bem-estar social. Esta pesquisa apresenta uma abordagem preditiva que combina dados demográficos desagregados por distrito, sexo e faixa etária com séries históricas de congestionamento urbano na cidade de São Paulo. A metodologia envolveu pré-processamento cuidadoso dos dados, codificação de variáveis categóricas e aplicação de cinco algoritmos de aprendizado supervisionado representando diferentes paradigmas metodológicos. A análise comparativa rigorosa dos modelos sob condições idênticas de dados e métricas estabelece benchmark para futuras pesquisas em previsão de tráfego urbano. Os algoritmos baseados em ensemble mostraram-se particularmente adequados para capturar as não linearidades inerentes ao fenômeno. O trabalho contribui para a literatura emergente sobre mobilidade urbana em megacidades de países em desenvolvimento, demonstrando metodologia replicável para integração de dados demográficos e padrões de tráfego em sistemas preditivos.
\end{abstract}

\begin{IEEEkeywords}
Predição de congestionamento, machine learning, dados populacionais, 
análise comparativa, random forest, tráfego urbano, série temporal, 
mobilidade urbana
\end{IEEEkeywords}

\section{Introdução}
O congestionamento urbano é um dos principais desafios enfrentados pelas grandes cidades contemporâneas. Intensificado pelo crescimento populacional acelerado e pela expansão desordenada da frota de veículos, esse fenômeno compromete diretamente a qualidade de vida, a eficiência econômica e a sustentabilidade ambiental \cite{1}. Estudos indicam que, em países desenvolvidos, os prejuízos causados pelo tráfego intenso somam bilhões de dólares por ano, refletindo em tempo perdido, aumento do consumo de combustível, elevação dos níveis de poluição e maior incidência de acidentes de trânsito. Na América Latina, o cenário é agravado pelo crescimento urbano desordenado e pela rápida motorização.

\subsection{Motivação}
Mitigar os impactos econômicos, sociais e ambientais do congestionamento é uma prioridade nas grandes metrópoles. A poluição, os atrasos nas viagens e os custos elevados de transporte exigem soluções eficazes e escaláveis. Antecipar cenários críticos de tráfego é essencial para otimizar a mobilidade e apoiar decisões em tempo real. Algoritmos de aprendizado de máquina desempenham papel significativo na análise de tráfego, permitindo identificar padrões complexos e prever situações de congestionamento \cite{2}. Quando aplicadas a dados históricos, essas técnicas viabilizam ações preventivas como ajustes semafóricos, orientação aos motoristas e melhorias no transporte público. Diversas abordagens têm demonstrado o potencial dessas técnicas para aumentar a eficiência do sistema viário.

\subsection{Problema}
Nas áreas urbanas, especialmente em grandes centros, o crescimento populacional e a motorização acelerada intensificam os congestionamentos, gerando impactos diretos na população e na economia. Entre os principais efeitos estão o aumento da poluição atmosférica e sonora, o maior consumo de combustível, a perda de tempo nas viagens e o crescimento das taxas de acidentes e infrações. Nos Estados Unidos, o congestionamento é apontado como uma ameaça ao desempenho econômico, enquanto na América Latina o problema se agrava com o crescimento desordenado das cidades.

\subsection{Objetivo}

Esta pesquisa busca comparar diferentes algoritmos de aprendizado de máquina para previsão do nível de congestionamento em vias urbanas da cidade de São Paulo, utilizando dados demográficos integrados a séries históricas de tráfego. O objetivo é identificar qual paradigma algorítmico melhor captura as relações não lineares dos padrões de congestionamento urbano, estabelecer suas configurações ótimas através de busca sistemática de hiperparâmetros, e oferecer fundação científica para sistemas de apoio à decisão em gestão inteligente do trânsito.

\subsection{Artefato}
Será desenvolvido um sistema de aprendizado de máquina treinado com dados históricos de tráfego e rótulos de congestionamento. Após o treinamento, o modelo receberá dados atuais como entrada e estimará o nível de congestionamento de curto prazo, disponibilizando as previsões em uma interface voltada ao monitoramento e à tomada de decisão antecipada.

\section{Estado da Arte}
O congestionamento urbano permanece como um dos desafios mais persistentes das grandes metrópoles contemporâneas, intensificado pelo crescimento populacional acelerado e pela expansão desordenada das áreas urbanas. Em São Paulo, esse fenômeno atinge dimensões críticas, com a cidade frequentemente figurando entre as mais congestionadas do mundo e registrando perdas econômicas estimadas em bilhões de reais anualmente \cite{3}.

Tradicionalmente, as abordagens para mitigação do congestionamento concentravam-se em soluções infraestruturais de larga escala, como a ampliação da malha viária, estratégias predominantes nas décadas de 1950 e 1960 sob a influência do modelo rodoviarista \cite{1}. Contudo, a constatação de que a ampliação da oferta viária frequentemente induz à demanda adicional por deslocamentos motorizados levou à necessidade de repensar essas abordagens.

A partir dos anos 1990, o debate passou a incorporar dimensões sociais e de equidade urbana. Pereira e Schwanen \cite{4} demonstraram que trabalhadores de baixa renda, residentes em regiões periféricas, gastam até 20\% mais tempo em deslocamentos diários quando comparados a indivíduos de maior poder aquisitivo. Essa desigualdade espacial evidencia a necessidade de compreender não apenas os padrões gerais de tráfego, mas também sua relação com características demográficas e distribuição populacional.

O congestionamento resulta de uma combinação complexa de fatores recorrentes e não recorrentes. Entre os fatores recorrentes, destacam-se o volume excessivo de veículos, a capacidade limitada das vias e os horários de pico. Já os fatores não recorrentes incluem incidentes, obras e condições climáticas adversas \cite{2}. Essa multiplicidade de causas demanda ferramentas analíticas sofisticadas, capazes de processar grandes volumes de dados heterogêneos e identificar padrões complexos.

Com o advento das tecnologias digitais e a consolidação da era do Big Data, novas perspectivas metodológicas emergiram. A disponibilidade crescente de dados georreferenciados, séries temporais de tráfego e informações demográficas detalhadas viabilizou a aplicação de técnicas de aprendizado de máquina para previsão de condições de tráfego. Diferentemente das abordagens baseadas em modelos físicos e simulações de engenharia, que demandam parametrizações complexas, os modelos orientados por dados têm demonstrado capacidade superior de generalização e adaptação a padrões emergentes \cite{5}.

Estudos recentes têm explorado diferentes arquiteturas de aprendizado de máquina aplicadas à previsão de congestionamentos, demonstrando que modelos baseados em ensemble, como Random Forest e Gradient Boosting, superam técnicas lineares tradicionais em termos de precisão preditiva, especialmente quando confrontados com padrões não lineares \cite{6}. A capacidade desses modelos de capturar relações hierárquicas torna-os particularmente adequados para contextos urbanos caracterizados por heterogeneidade espacial e temporal.

A regressão linear, apesar de sua simplicidade, permanece relevante como baseline metodológico. Algoritmos baseados em instâncias, como K-Nearest Neighbors, têm sido aplicados devido à sua capacidade de capturar padrões locais, embora apresentem limitações de custo computacional em datasets volumosos. As técnicas de ensemble, representadas por Random Forest e XGBoost, têm alcançado desempenho superior em diversos problemas de previsão de tráfego, apresentando robustez contra overfitting e eficiência computacional em datasets de grande escala.

As redes neurais artificiais, em especial os Multilayer Perceptrons, constituem outra classe relevante de modelos. Embora exijam maiores volumes de dados para treinamento e apresentem menor interpretabilidade, oferecem flexibilidade arquitetural que permite modelar interações de alta ordem entre variáveis.

Um aspecto crítico na aplicação de técnicas de aprendizado de máquina é a capacidade de explicar as predições geradas. Técnicas de interpretabilidade como SHAP têm sido crescentemente adotadas para quantificar a contribuição individual de cada variável nas previsões de modelos complexos, permitindo que gestores urbanos compreendam quais fatores exercem maior influência sobre os padrões de congestionamento \cite{7}. Essa transparência metodológica é fundamental para a validação científica dos modelos e para a construção de confiança nas ferramentas computacionais.

A relação entre características populacionais e padrões de congestionamento constitui uma linha de investigação particularmente relevante, mas ainda pouco explorada. Enquanto estudos tradicionais concentram-se em variáveis diretas de tráfego, a incorporação de dados demográficos desagregados por região, faixa etária e gênero pode revelar padrões estruturais subjacentes que refletem a organização espacial das atividades urbanas.

No contexto específico de São Paulo, a heterogeneidade demográfica entre os diferentes distritos, combinada com a desigual distribuição de empregos formais e infraestrutura de transporte, sugere que modelos preditivos podem beneficiar-se significativamente da incorporação de variáveis populacionais. A modelagem conjunta dessas características demográficas com séries históricas de congestionamento permite não apenas prever condições futuras de tráfego, mas também identificar áreas prioritárias para intervenções de planejamento urbano.

Diante desse panorama, a presente pesquisa explora especificamente a capacidade de diferentes técnicas de aprendizado de máquina em prever padrões de congestionamento a partir de características populacionais desagregadas por distrito, sexo e faixa etária. A proposta metodológica enfatiza a comparação de desempenho preditivo entre os modelos por meio de métricas consolidadas (MAE, RMSE e R²), além da análise de importância de features quando disponível nos algoritmos baseados em árvores, visando identificar quais variáveis demográficas exercem maior influência nos padrões de congestionamento observados na cidade de São Paulo.

\section{Metodologia}
A metodologia adotada nesta pesquisa foi estruturada em cinco etapas principais: aquisição de dados, pré-processamento, modelagem, avaliação e interpretação dos resultados. O pipeline seguido está ilustrado na Figura \ref{fig:pipeline}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.15\textwidth]{pipeline.png}
\caption{Pipeline da metodologia}
\label{fig:pipeline}
\end{figure}

Inicialmente, o estudo partiu de um dataset referente ao trânsito da cidade de São Paulo. Para enriquecer a análise e estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da capital, foram buscados dados complementares no Portal de Dados Abertos do Estado de São Paulo \cite{8}. Desse portal, foi extraído um segundo dataset contendo informações sobre a população de São Paulo ao longo dos anos.

Na segunda etapa, foi realizada a limpeza e padronização dos dados. Os datasets apresentavam ruídos, campos fora de padrão e diferentes codificações de texto (encodings), o que exigiu um processo cuidadoso de tratamento para garantir a compatibilidade e integridade das informações. Após a unificação dos dados, com as variáveis devidamente estruturadas, a pesquisa avançou para a etapa de modelagem com algoritmos de aprendizado de máquina.

Na terceira etapa, foram implementados e treinados diferentes modelos de machine learning, cada um com suas configurações e hiperparâmetros específicos. A quarta etapa consistiu na execução dos scripts desenvolvidos para obtenção dos valores das métricas de desempenho de cada modelo. Por fim, a quinta etapa envolveu a interpretação e análise comparativa dos resultados obtidos, permitindo avaliar a eficácia dos algoritmos frente aos dados tratados e compreender melhor a relação entre o crescimento populacional e o impacto no trânsito da cidade.

\subsection{Aquisição de Dados}\
O ponto de partida da pesquisa foi a seleção de um dataset sobre o trânsito da cidade de São Paulo. Este conjunto de dados se mostrou especialmente relevante por conter informações como dia, hora, região e, principalmente, o nível de congestionamento, variável central para os objetivos do estudo. O dataset original contém 77.159 registros com as seguintes variáveis: identificador único, data, hora, nome da via, direção, via expressa, tamanho do congestionamento em metros e região da cidade. Algumas colunas adicionais estavam presentes, mas foram descartadas por não contribuírem diretamente para a análise proposta.

Para estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da cidade, foi incorporado um segundo dataset, obtido por meio do Portal de Dados Abertos do Estado de São Paulo \cite{8}. Este conjunto (\texttt{estimativa\_pop\_idade\_sexo\_msp.csv}) trazia dados populacionais da capital paulista ao longo dos anos, incluindo as variáveis ano, código do distrito, nome do distrito, sexo, faixa etária e população total. O dataset original contém 81.601 registros distribuídos pelos distritos do município.

Apesar da riqueza informacional, esse segundo dataset apresentou desafios técnicos. Havia ruídos nos dados, campos fora de padrão e codificações de texto distintas (encodings), o que exigiu um esforço adicional para uniformização e tratamento adequado. Um ponto específico que merece destaque é a estrutura da variável idade, que estava agrupada em faixas etárias (por exemplo, "10 a 14 anos", "15 a 19 anos"). Essa segmentação dificultou a filtragem precisa de indivíduos com idade mínima para conduzir veículos (a partir dos 18 anos), sendo possível apenas considerar faixas a partir dos 20 anos, o que introduz um pequeno desvio na análise.

A aquisição dos dados, portanto, envolveu não apenas a seleção criteriosa dos conjuntos mais relevantes, mas também a antecipação dos desafios que seriam enfrentados nas etapas seguintes de tratamento e modelagem. A escolha destes datasets foi motivada pela relevância contextual, considerando que São Paulo enfrenta problemas crônicos de congestionamento com impactos econômicos e sociais significativos, pela confiabilidade das fontes oficiais com sistemas de monitoramento consolidados, e pela complementaridade entre dados de tráfego e demográficos, permitindo investigar a hipótese de relação entre crescimento populacional e padrões de congestionamento.

As variáveis de entrada (features) selecionadas foram: população total, hora do dia em formato numérico (0-23), via expressa codificada, região codificada, distribuição de sexo codificada, dia da semana (0-6) e mês (1-12). As features temporais dia da semana e mês foram derivadas posteriormente a partir da coluna de data durante o processo de preparação dos dados para cada modelo. A variável de saída (target) definida foi o tamanho do congestionamento em metros.

\subsection{Limpeza e Preparação}
Para assegurar a consistência e a qualidade dos dados utilizados na modelagem preditiva, foram desenvolvidos dois pipelines automatizados de pré-processamento, um voltado ao tratamento dos dados populacionais e outro dedicado ao conjunto de dados de congestionamentos de tráfego. Ambos os processos tiveram como objetivo padronizar formatos, remover inconsistências e preparar os dados para a etapa de integração e posterior aplicação de algoritmos de aprendizado de máquina.

O conjunto populacional passou por um tratamento detalhado executado por script Python desenvolvido especificamente para este fim. Inicialmente, todos os campos textuais foram normalizados por meio da remoção de acentuação e da padronização de espaços, garantindo uniformidade entre distritos e categorias. Em seguida, foi aplicado um filtro para manter apenas as faixas etárias com idade igual ou superior a 20 anos, por conta de limitações do dataset, sendo esta a aproximação mais próxima da população apta a conduzir veículos. Na sequência, foi criada uma nova variável denominada região, obtida a partir do mapeamento de cada distrito para uma das cinco grandes áreas geográficas de São Paulo: norte, sul, leste, oeste e centro. Esse mapeamento foi implementado por meio de um dicionário abrangendo os 96 distritos oficiais do município, permitindo uma agregação coerente com as divisões espaciais utilizadas nos dados de tráfego. Por fim, distritos que não possuíam correspondência foram identificados e registrados em um arquivo auxiliar para verificação manual, garantindo a completude do mapeamento antes da exportação final dos dados, que foram salvos em formato CSV com codificação UTF-8 padronizada.

O conjunto de dados de tráfego, processado por script Python, exigiu um tratamento voltado principalmente à validação e à consistência das informações temporais e geográficas. As etapas iniciais incluíram a normalização dos campos textuais referentes a vias, regiões e expressways, removendo acentuação e espaços redundantes. Em seguida, foi realizada a validação das variáveis de data e hora, assegurando que apenas registros com formato temporal correto fossem mantidos. O campo de tamanho do congestionamento também passou por verificação, sendo eliminados valores negativos, nulos ou não numéricos, etapa considerada crítica pois a variável target precisa apresentar qualidade máxima para garantir a confiabilidade dos modelos. As regiões foram padronizadas para um formato em letras minúsculas e comparadas a uma lista de referência contendo as mesmas categorias usadas no dataset populacional (norte, sul, leste, oeste e centro), garantindo a compatibilidade entre as bases. Registros duplicados foram identificados e removidos através de comparação de chaves compostas (data, hora, via, região), e, ao final, foi gerado um relatório estatístico que resumiu a distribuição dos congestionamentos por região, além de medidas descritivas como média, mediana e desvio padrão do tamanho dos congestionamentos.

Ambos os conjuntos de dados resultantes foram padronizados quanto à codificação e ao delimitador, de forma a assegurar compatibilidade durante o processo de integração. A técnica de Label Encoding foi aplicada para transformar variáveis categóricas (região, via expressa, sexo) em valores numéricos ordinais, tornando-as adequadas para processamento pelos algoritmos de aprendizado de máquina. Os dados foram então agregados por data, hora, via e região para reduzir redundâncias e consolidar as informações. Registros com valores ausentes em features consideradas críticas foram removidos, priorizando a qualidade dos dados sobre a quantidade. Valores negativos de congestionamento foram removidos por representarem inconsistência física, enquanto outliers extremos foram identificados mas mantidos no dataset, pois podem representar situações reais de congestionamento severo que são relevantes para o modelo.

Essa padronização foi fundamental para que os datasets pudessem ser combinados por meio das variáveis região e ano, possibilitando análises conjuntas sobre o impacto da densidade populacional no comportamento do tráfego e permitindo que as etapas seguintes de modelagem preditiva fossem conduzidas de maneira consistente e reprodutível.

\subsection{Seleção de Modelos}
Foram selecionados cinco algoritmos de aprendizado de máquina supervisionado para avaliação comparativa, representando diferentes paradigmas de modelagem: Regressão Linear (modelo linear tradicional), K-Nearest Neighbors (modelo baseado em instâncias), Random Forest (ensemble de árvores de decisão), Multilayer Perceptron (rede neural artificial) e XGBoost (método de boosting baseado em árvores de decisão). Esta diversidade permite comparar abordagens com diferentes premissas teóricas, capacidades de modelagem e complexidades computacionais. Os algoritmos foram implementados utilizando a biblioteca scikit-learn para Python 3.13, com exceção do XGBoost que utiliza sua biblioteca própria (xgboost). Adicionalmente, foram utilizadas as bibliotecas pandas para manipulação de dados, numpy para operações numéricas e matplotlib para visualizações. O ajuste de hiperparâmetros foi realizado através de experimentação empírica sistemática, priorizando configurações que apresentassem convergência estável e equilíbrio entre desempenho preditivo e custo computacional.

\subsection{Métricas de Avaliação}
Para avaliar o desempenho dos modelos, foram utilizadas três métricas apropriadas para problemas de regressão. O Mean Absolute Error (MAE), calculado como $\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$, onde $y_i$ representa o valor real, $\hat{y}_i$ o valor predito e $n$ o número de amostras, representa o erro médio absoluto em metros, sendo robusta a outliers e de interpretação direta. O Root Mean Squared Error (RMSE), definido como $\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$, também expressa o erro em metros, mas penaliza mais fortemente erros grandes devido ao termo quadrático, sendo que um RMSE significativamente maior que o MAE indica presença de outliers ou erros com alta variabilidade. O Coeficiente de Determinação ($R^2$), calculado como $R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$, onde $\bar{y}$ é a média dos valores reais, indica a proporção da variância da variável target que é explicada pelo modelo, variando entre 0 e 1 (podendo ser negativa para modelos inadequados), sendo que valores próximos a 1 indicam melhor capacidade preditiva.

\subsection{Avaliação dos Modelos}
Após a definição e implementação dos modelos de aprendizado de máquina, foi realizada a etapa de avaliação através da execução dos scripts desenvolvidos para cada algoritmo. Nesta etapa, os modelos foram treinados com os dados de treinamento e, em seguida, suas predições foram comparadas com os valores reais tanto no conjunto de treinamento quanto no conjunto de teste. Para cada modelo e configuração testada, foram registrados os valores das métricas MAE, RMSE e $R^2$, permitindo quantificar objetivamente o desempenho preditivo. Os resultados foram armazenados de forma estruturada para posterior análise comparativa. Esta etapa também incluiu a verificação de possíveis casos de overfitting, identificados quando o desempenho no conjunto de treinamento é substancialmente superior ao desempenho no conjunto de teste, indicando que o modelo memorizou os dados de treinamento ao invés de aprender padrões generalizáveis.

\section{Resultados}
A avaliação sistemática dos algoritmos selecionados revelou desempenho substancialmente heterogêneo na previsão de congestionamento urbano em São Paulo. Os resultados evidenciam que modelos baseados em ensemble, particularmente Random Forest e XGBoost, demonstraram capacidade superior de capturar as não linearidades e interações complexas presentes nos dados demográficos e temporais. Os métodos baseados em árvores de decisão alcançaram coeficientes de determinação superiores a 0.60, explicando mais de 60\% da variância observada no congestionamento, enquanto técnicas mais simples como regressão linear apresentaram limitações estruturais evidentes.

\subsection{K-Nearest Neighbors}

O algoritmo K-Nearest Neighbors fundamenta-se no princípio de aprendizado baseado em instâncias, onde predições são derivadas através da agregação dos k vizinhos mais próximos no espaço de features. Este método não paramétrico demonstrou desempenho moderado na previsão de congestionamento urbano, alcançando coeficiente de determinação de 0,56 na melhor configuração testada, com erro médio absoluto de 749,3 metros, evidenciando sua capacidade de capturar heterogeneidades espaciais e temporais sem imposição de estrutura funcional global.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular}{ccccccccc}
\hline
\textbf{Estratégia} & \textbf{Distância} & \textbf{Pesos} & \textbf{k} & \textbf{MAE (m)} & \textbf{RMSE (m)} & \textbf{R²} \\
\hline
Holdout & Minkowski & Uniform & 5 & 953,9 & 1504,2 & 0,49 \\
K-Fold (10) & Manhattan & Distance & 7 & \textbf{749,3} & --- & \textbf{0,56} \\
Holdout & Euclidean & Distance & 3 & 793,9 & 1464,8 & 0,52 \\
K-Fold (5) & Chebyshev & Uniform & 8 & 974,9 & --- & 0,48 \\
Holdout & Chebyshev & Uniform & 6 & 966,1 & 1513,3 & 0,48 \\
\hline
\end{tabular}
\caption{Resultados das configurações do modelo K-Nearest Neighbors}
\label{tab:resultados_knn}
\end{table}

A configuração KNN \#1, utilizando estratégia holdout com divisão 75/25, distância de Minkowski e pesos uniformes com k igual a 5, apresentou desempenho moderado com MAE de 953,9 metros e R² de 0,49. O modelo explicou aproximadamente metade da variância observada no congestionamento, demonstrando adequação parcial da abordagem baseada em instâncias. A diferença contida entre desempenho de treino (MAE = 757,3 m, R² = 0,67) e teste sugere ausência de overfitting severo.

A configuração KNN \#2 apresentou o melhor desempenho global entre todas as variações testadas, alcançando MAE de 749,3 metros e R² de 0,56 através de validação cruzada com 10 folds. A utilização da distância de Manhattan combinada com pesos baseados em distância e k igual a 7 mostrou-se particularmente adequada para este problema, com baixa variabilidade entre folds (desvio padrão de 5,2 metros no MAE), confirmando a estabilidade e consistência desta configuração na captura de padrões generalizáveis nos dados de congestionamento.

O modelo KNN \#3 explorou a combinação de distância Euclidiana com pesos baseados em distância e k igual a 3, resultando em MAE de 793,9 metros e R² de 0,52. Embora tenha apresentado desempenho inferior ao KNN \#2, o modelo demonstrou excelente capacidade no conjunto de treinamento (R² = 0,86), indicando que valores reduzidos de k capturam nuances locais mas tendem a memorizar ruídos do treinamento, refletido na discrepância significativa entre treino e teste.

As configurações KNN \#4 e KNN \#5 testaram a métrica de Chebyshev, que considera apenas a maior diferença entre dimensões. Ambas apresentaram desempenho inferior com R² aproximadamente 0,48, sugerindo inadequação desta métrica para capturar a estrutura multidimensional do espaço de features no problema de congestionamento urbano. A superioridade consistente da distância de Manhattan sobre as demais métricas evidencia que, em dados urbanos heterogêneos, métricas que tratam diferenças lineares em cada dimensão são mais apropriadas que penalizações quadráticas ou comparações de máximo absoluto.

\subsection{Regressão Linear}

A regressão linear, técnica fundamental de modelagem estatística, estabelece relações lineares entre variáveis preditoras e variável dependente através de minimização de erros quadráticos. Nesta aplicação de previsão de congestionamento urbano, o método apresentou desempenho substancialmente limitado, com coeficiente de determinação de apenas 0,10, explicando apenas 10\% da variância observada. Este resultado confirma empiricamente que as relações entre características demográficas, temporais e espaciais com congestionamento são fundamentalmente não lineares e envolvem interações complexas que não podem ser capturadas por combinações aditivas simples de features.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular}{cccccc}
\hline
\textbf{Estratégia} & \textbf{Intercepto} & \textbf{MAE (m)} & \textbf{RMSE (m)} & \textbf{R²} \\
\hline
Holdout & Sim & 1379,7 & 1990,8 & 0,10 \\
Holdout & Não & 2443,8 & 3133,9 & -1,21 \\
K-Fold (10) & Sim & 1377,9 & --- & 0,10 \\
Holdout & Sim & 1379,7 & 1990,8 & 0,10 \\
K-Fold (7) & Sim & 1377,9 & --- & 0,10 \\
\hline
\end{tabular}
\caption{Resultados das configurações do modelo de Regressão Linear}
\label{tab:resultados_linear}
\end{table}

A configuração Linear \#1, estabelecendo baseline metodológico com estratégia holdout, todas as variáveis e inclusão do intercepto, apresentou R² de 0,10 e MAE de 1379,7 metros. O modelo explicou apenas 10\% da variância, significativamente inferior aos métodos não lineares posteriormente testados, evidenciando a predominância de relações não lineares e interações complexas no fenômeno de congestionamento urbano. A diferença moderada entre desempenho de treino e teste indica ausência de overfitting, mas simultaneamente revela limitação estrutural na captura dos padrões subjacentes.

A configuração Linear \#2, que removeu o intercepto, resultou em deterioração drástica com R² de -1,21 e MAE de 2443,8 metros. Valores negativos de R² indicam predições piores que simplesmente utilizar a média como estimativa, demonstrando completa inadequação desta especificação e confirmando que o termo independente é componente essencial do modelo, representando o nível basal de congestionamento.

As configurações Linear \#3 e Linear \#5 empregaram validação cruzada com 10 e 7 folds respectivamente, apresentando resultados praticamente idênticos ao baseline com R² de aproximadamente 0,10 e MAE de 1377,9 metros. A baixíssima variabilidade entre folds (desvio padrão de 6 metros) atesta estabilidade estatística do modelo, mas simultaneamente confirma sua limitação estrutural intrínseca. A consistência dos resultados através de diferentes partições dos dados elimina a possibilidade de que o baixo desempenho seja artefato da amostragem específica.

O papel da regressão linear nesta pesquisa consolida-se como baseline metodológico essencial, estabelecendo patamar mínimo de desempenho e demonstrando empiricamente a inadequação da linearidade. O contraste marcante entre o R² de 0,10 da regressão linear e os 0,49-0,62 alcançados pelos demais modelos quantifica objetivamente o ganho proporcionado por técnicas capazes de capturar não linearidades e interações complexas nos dados de congestionamento urbano.

\subsection{Multilayer Perceptron}

O Multilayer Perceptron, representando arquitetura fundamental de redes neurais artificiais, demonstra capacidade teórica de aproximar universalmente funções contínuas através de múltiplas camadas de neurônios com funções de ativação não lineares. Nesta pesquisa, o modelo apresentou desempenho intermediário com coeficiente de determinação de 0,50 na melhor configuração, explicando metade da variância do congestionamento com erro médio absoluto de 962,3 metros, demonstrando eficácia razoável na captura de relações não lineares mas inferior aos métodos baseados em árvores de decisão.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cccccccc}
\hline
\textbf{Estratégia} & \textbf{Arquitetura} & \textbf{Ativação} & \textbf{Otim.} & \textbf{MAE (m)} & \textbf{RMSE (m)} & \textbf{R²} \\
\hline
Holdout & (100,50,25) & ReLU & Adam & 988,5 & 1531,6 & 0,47 \\
Holdout & (256,128,64,32) & ReLU & Adam & \textbf{962,3} & \textbf{1490,8} & \textbf{0,50} \\
Holdout & (64,32) & ReLU & Adam & 1005,1 & 1537,1 & 0,47 \\
K-Fold (5) & (100,50) & ReLU & Adam & 988,4 & --- & 0,47 \\
K-Fold (5) & (168,64,32) & Tanh & SGD & 1502,7 & --- & 0,00 \\
\hline
\end{tabular}
\caption{Resultados das configurações do Multilayer Perceptron}
\label{tab:resultados_mlp}
\end{table}

A configuração MLP \#1 estabeleceu baseline com arquitetura de três camadas ocultas (100, 50, 25 neurônios), função de ativação ReLU e otimizador Adam, convergindo em 179 iterações e alcançando R² de 0,47 com MAE de 988,5 metros. Este desempenho equipara-se ao obtido pelo KNN baseline, sugerindo que ambos os paradigmas (aprendizado baseado em instâncias versus aproximação funcional por redes neurais) capturam porções similares da variância nos dados, embora nenhum deles explore plenamente as não linearidades presentes.

A configuração MLP \#2 explorou arquitetura substancialmente mais profunda com quatro camadas ocultas (256, 128, 64, 32 neurônios), produzindo o melhor desempenho entre as configurações MLP com MAE de 962,3 metros e R² de 0,50. O incremento de aproximadamente 26 metros no MAE e 0,03 no R² comparado ao baseline demonstra que o aumento da profundidade permitiu capturar padrões mais sutis nos dados através de representações hierárquicas progressivamente mais ricas, com regularização L2 prevenindo overfitting apesar da maior complexidade arquitetural.

O modelo MLP \#3 testou arquitetura compacta com apenas duas camadas ocultas (64, 32 neurônios) e convergiu em 212 iterações, mantendo desempenho comparável ao baseline com R² de 0,47 e MAE aproximadamente 1005 metros. Este resultado sugere trade-off favorável entre complexidade e desempenho, onde configurações intermediárias podem oferecer equilíbrio adequado para aplicações práticas com restrições computacionais, embora a redução de camadas limite a profundidade das representações hierárquicas aprendidas.

A configuração MLP \#4 empregou validação cruzada 5-fold com arquitetura intermediária (100, 50 neurônios), obtendo R² médio de 0,47 com desvio padrão de 0,005 e MAE de 988,4 metros. A baixa variabilidade entre folds confirma estabilidade da rede neural através de diferentes partições dos dados, validando que o desempenho observado não é artefato de uma divisão específica mas representa generalização genuína dos pesos aprendidos.

A configuração MLP \#5 testou combinação alternativa de função de ativação tangente hiperbólica e otimizador SGD clássico, resultando em falha completa de convergência com R² aproximadamente 0,00 e MAE superior a 1500 metros. Este resultado confirma empiricamente a superioridade da combinação ReLU-Adam, onde a função tanh apresenta saturação em ambas as extremidades causando gradientes muito pequenos durante retropropagação, combinada com otimizador SGD sem mecanismos adaptativos que não conseguiu compensar este problema em redes profundas.

\subsection{Random Forest}

O algoritmo Random Forest, técnica de ensemble que agrega múltiplas árvores de decisão treinadas independentemente sobre subconjuntos dos dados, demonstrou superior capacidade de previsão de congestionamento urbano nesta pesquisa. O modelo alcançou o melhor desempenho global com coeficiente de determinação de 0,62 e erro médio absoluto de 760,2 metros, explicando aproximadamente dois terços da variância observada, evidenciando eficácia notável em capturar não linearidades, interações entre variáveis e heterogeneidades espaciais característica de padrões de tráfego urbano.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cccccccc}
\hline
\textbf{Estratégia} & \textbf{Árvores} & \textbf{Prof. Máx.} & \textbf{MAE (m)} & \textbf{RMSE (m)} & \textbf{R²} \\
\hline
Holdout & 100 & Sem limite & \textbf{760,2} & 1298,6 & \textbf{0,62} \\
Holdout & 200 & 30 & 749,7 & 1439,9 & 0,53 \\
Holdout & 300 & 15 & 857,9 & 1348,9 & 0,59 \\
K-Fold (5) & 50 & 10 & 974,8 & --- & 0,49 \\
K-Fold (5) & 150 & 12 & 931,0 & --- & 0,53 \\
\hline
\end{tabular}
\caption{Resultados das configurações do Random Forest}
\label{tab:resultados_rf}
\end{table}

A configuração RF \#1, estabelecendo baseline com 100 árvores e ausência de limitação de profundidade, alcançou o melhor desempenho global entre todas as configurações testadas na pesquisa com R² de 0,62 e MAE de 760,2 metros. Este resultado representa melhoria substancial sobre KNN (R² = 0,56) e MLP (R² = 0,50), evidenciando eficácia do ensemble em capturar complexidades não lineares do congestionamento urbano. A análise de importância de features revelou população total como contribuinte dominante com 37\% da capacidade preditiva, seguida por via expressa (15\%), mês (15\%) e hora do dia (13\%).

A configuração RF \#2, com 200 árvores e profundidade limitada a 30 níveis, produziu MAE de 749,7 metros mas R² de apenas 0,53, inferior ao baseline apesar do erro médio ligeiramente menor. A discrepância significativa entre desempenho de treino (R² = 0,87, MAE = 327 m) e teste indica overfitting moderado, sugerindo que a combinação de muitas árvores com profundidade elevada permitiu memorização de idiossincrasias dos dados de treinamento. A redução no R² apesar do MAE competitivo reflete captura inadequada da variância global do fenômeno.

O modelo RF \#3 testou configuração robusta com 300 árvores mas profundidade máxima restrita a 15 níveis, buscando equilíbrio entre complexidade e generalização. O resultado de R² de 0,59 e MAE de 857,9 metros representa compromisso intermediário onde a limitação de profundidade previne overfitting mas também restringe capacidade de capturar padrões muito específicos. A distribuição de importância de features mostrou-se mais balanceada, com população total (36\%), via expressa (21\%) e região (18\%) dominando, sugerindo que restrição de profundidade força utilização mais uniforme de todas as features disponíveis.

As configurações RF \#4 e RF \#5 empregaram validação cruzada 5-fold com arquiteturas mais parcimoniosas. O RF \#4, com apenas 50 árvores e profundidade máxima de 10, alcançou R² médio de 0,49 com desvio padrão de 0,002, demonstrando alta estabilidade através dos folds e oferecendo vantagens computacionais significativas, sendo adequado para prototipagem rápida ou cenários com restrições de recursos. O RF \#5, com 150 árvores e profundidade 12, obteve melhor equilíbrio com R² médio de 0,53 e MAE de 931 metros, confirmando que configurações intermediárias com regularização estrutural produzem modelos mais estáveis.

\subsection{XGBoost}

O algoritmo XGBoost, implementação otimizada de gradient boosting que constrói modelos sequencialmente corrigindo erros residuais das árvores anteriores, apresentou desempenho competitivo com o Random Forest. O modelo alcançou coeficiente de determinação de 0,60 e erro médio absoluto de 844,7 metros na melhor configuração testada, evidenciando eficácia comparable ao Random Forest com vantagens adicionais em controle fino de hiperparâmetros, treinamento incremental e velocidade de inferência. O algoritmo combina estratégias de regularização explícita, aproximação por histograma e paralelização automática, resultando em capacidade preditiva robusta com menor requisito computacional que Random Forest.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccccccc}
\hline
\textbf{Estratégia} & \textbf{Árvores} & \textbf{Prof.} & \textbf{LR} & \textbf{MAE (m)} & \textbf{RMSE (m)} & \textbf{R²} \\
\hline
Holdout & 100 & 6 & 0,10 & 953,1 & 1468,7 & 0,51 \\
Holdout & 400 & 10 & 0,03 & \textbf{844,7} & \textbf{1327,3} & \textbf{0,60} \\
K-Fold (5) & 80 & 5 & 0,02 & 1124,3 & --- & 0,36 \\
K-Fold (5) & 300 & 8 & 0,05 & 913,0 & --- & 0,55 \\
Holdout & 50 & 4 & 0,30 & 983,6 & 1507,5 & 0,49 \\
\hline
\end{tabular}
\caption{Resultados das configurações do XGBoost}
\label{tab:resultados_xgb}
\end{table}

A configuração XGB \#1 estabeleceu baseline com hiperparâmetros padrão da biblioteca (100 árvores, profundidade 6, learning rate 0,1), alcançando R² de 0,51 e MAE de 953,1 metros. Este desempenho supera marginalmente a regressão linear (R² = 0,10) mas situa-se abaixo do KNN (R² = 0,56) e Random Forest (R² = 0,62), sugerindo que a configuração padrão subotimiza as capacidades do algoritmo para este problema específico. O tempo de execução reduzido comparado ao Random Forest o torna adequado para prototipagem rápida e estabelecimento de linhas de base.

A configuração XGB \#2 produziu desempenho altamente competitivo com R² de 0,60 e MAE de 844,7 metros, representando o melhor resultado XGBoost testado. Esta especificação empregou 400 árvores com profundidade máxima de 10 e taxa de aprendizado reduzida para 0,03, implementando estratégia de boosting profundo com aprendizado conservador. O número elevado de árvores combinado com learning rate baixo permite construção gradual de modelo complexo onde cada árvore contribui modestamente, reduzindo overfitting através de agregação de múltiplas predições fracas. A profundidade de 10 níveis possibilita captura de interações de alta ordem enquanto regularização L2 padrão previne ajuste excessivo aos dados de treinamento.

A configuração XGB \#3 explorou extremo de regularização máxima com apenas 80 árvores rasas (profundidade 5) e learning rate mínimo (0,02), priorizando velocidade computacional e forte restrição de complexidade. O resultado de R² de 0,36 e MAE de 1124,3 metros, substancialmente inferior aos demais modelos, demonstra que regularização excessiva pode degradar capacidade preditiva ao ponto de underfitting severo. A consistência entre folds confirma estabilidade mas revela que o modelo está insuficientemente complexo para capturar padrões não lineares nos dados.

A configuração XGB \#4 empregou validação cruzada com 300 árvores, profundidade 8 e learning rate intermediário (0,05), alcançando equilíbrio entre complexidade e generalização com R² médio de 0,55 e MAE de 913,0 metros. A baixa variabilidade entre folds (desvio padrão de 6,6 metros) atesta robustez do modelo, tornando esta configuração particularmente adequada para implantação em produção onde consistência de predições é prioritária. O subsampling agressivo de 70\% dos dados e features em cada iteração promove diversidade entre árvores sucessivas, reduzindo correlação e melhorando generalização.

O modelo XGB \#5 testou configuração minimalista com 50 árvores de profundidade 4 e learning rate elevado (0,30), voltada para inferência em tempo real com severas restrições computacionais. O desempenho de R² de 0,49 e MAE de 983,6 metros, embora inferior aos modelos mais complexos, demonstra que XGBoost mantém capacidade preditiva razoável mesmo em configurações altamente restritas. O learning rate alto acelera convergência mas aumenta risco de overshooting, explicando o desempenho intermediário e tornando esta configuração apropriada para sistemas embarcados ou aplicações móveis.

\section{Conclusões}
A presente pesquisa conduziu análise comparativa sistemática de cinco algoritmos de aprendizado de máquina aplicados à previsão de congestionamento urbano em São Paulo, utilizando dados demográficos integrados a séries históricas de tráfego cobrindo o período de 2018 a 2024. Este intervalo temporal compreende integralmente o período da pandemia de COVID-19 (2020-2022), durante o qual alterações drásticas nos padrões de mobilidade urbana ocorreram em consequência de lockdowns, trabalho remoto massivo e redução acentuada da circulação de veículos. A capacidade dos modelos de manter desempenho razoável mesmo com esta descontinuidade estrutural nas séries temporais atesta robustez dos algoritmos testados, particularmente dos métodos ensemble que demonstraram maior resistência a mudanças abruptas de regime nos padrões de tráfego.

Os resultados demonstram que técnicas de ensemble baseadas em árvores de decisão superam substancialmente abordagens lineares e baseadas em instâncias. O Random Forest alcançou o melhor desempenho com R² de 0,62 e MAE de 760 metros, enquanto o XGBoost apresentou desempenho competitivo com R² de 0,60 e MAE de 845 metros. O K-Nearest Neighbors atingiu R² de 0,56 com MAE de 749 metros, o Multilayer Perceptron resultou em R² de 0,50 com MAE de 962 metros, e a Regressão Linear produziu o pior desempenho com R² de 0,10 e MAE de 1380 metros, estabelecendo baseline que evidencia a predominância de relações não lineares no fenômeno estudado. A superioridade dos métodos baseados em árvores fundamenta-se em sua capacidade natural de capturar interações complexas entre variáveis, agregar predições reduzindo variância através de ensembles, e apresentar robustez a outliers e invariância a transformações monotônicas das features.

Contudo, a presente pesquisa operou através de experimentação empírica, testando configurações de hiperparâmetros selecionadas manualmente sem busca sistemática no espaço de configurações potenciais. Para maximizar o desempenho preditivo dos modelos desenvolvidos, implementação de algoritmos de busca local como Simulated Annealing e Hill Climbing constituem estratégias fundamentadas teoricamente que poderiam superar significativamente os resultados obtidos, forneceriam fundamentação matemática sólida para otimização de hiperparâmetros ao invés da abordagem empírica atual.

A incorporação de variáveis contextuais adicionais representa avenida complementar para aprimoramento. Dados climáticos, indicadores de eventos especiais e calendário de feriados capturariam fatores explanatórios para porções da variabilidade atualmente tratadas como ruído aleatório. Engenharia de features sofisticada e ensemble heterogêneo combinando predições de múltiplos algoritmos com diferentes vieses indutivos igualmente ofereceriam oportunidades de ganho. Segmentação do problema através de modelagem hierárquica especializada por região ou horário, coleta de dados com maior granularidade temporal e espacial, e integração de múltiplas fontes (sensores IoT, GPS de aplicativos de mobilidade, redes sociais) em sistemas preditivos multimodais representam direções naturais para pesquisas futuras.

Os resultados obtidos estabelecem que algoritmos de aprendizado de máquina, particularmente Random Forest e XGBoost, são ferramentas viáveis e efetivas para previsão de congestionamento urbano utilizando dados demográficos e temporais acessíveis através de fontes oficiais. A capacidade de explicar 60-62\% da variância no congestionamento fornece base sólida para sistemas de apoio à decisão em gestão de tráfego e planejamento urbano, embora oportunidades significativas permaneçam para otimização sistemática de hiperparâmetros através de algoritmos de busca local e incorporação de variáveis contextuais complementares. A metodologia estabelecida é replicável para outros contextos urbanos e fornece contribuição substancial à compreensão das dinâmicas de mobilidade metropolitana através de integração entre dados demográficos e séries de tráfego.

\begin{thebibliography}{00}

\bibitem{1} G. S. B. Vianna and C. Young, ``Em busca do tempo perdido: uma estimativa do produto perdido em trânsito no Brasil,'' \textit{Revista de Economia Contemporânea}, vol. 19, no. 3, pp. 403--416, set.--dez. 2015.
\bibitem{2} M. Akhtar and S. Moridpour, ``A review of traffic congestion prediction using artificial intelligence,'' \textit{Journal of Advanced Transportation}, vol. 2021, pp. 1--18, 2021.
\bibitem{3} D. B. Vale, ``The welfare costs of traffic congestion in São Paulo Metropolitan Area,'' Tese (Doutorado), Universidade de São Paulo, São Paulo, 2018.
\bibitem{4} R. H. M. Pereira and T. Schwanen, ``Tempo de deslocamento casa-trabalho no Brasil (1992-2009): diferenças entre regiões metropolitanas, níveis de renda e sexo,'' Texto para Discussão, IPEA, Brasília, n. 1813, 2013.
\bibitem{5} E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, ``Short-term traffic forecasting: where we are and where we're going,'' \textit{Transportation Research Part C: Emerging Technologies}, vol. 43, pp. 3--19, 2014.
\bibitem{6} P. Zechin et al., ``Traffic congestion prediction using machine learning techniques,'' in \textit{Proceedings of the 11th Brazilian Conference on Intelligent Systems}, Campinas: SBC, 2022, pp. 214--225.
\bibitem{7} S. M. Lundberg and S.-I. Lee, ``A unified approach to interpreting model predictions,'' in \textit{Proceedings of the 31st Conference on Neural Information Processing Systems}, Long Beach: NIPS, 2017, pp. 4765--4774.
\bibitem{8} Governo do Estado de São Paulo, ``Portal de Dados Abertos do Estado de São Paulo,'' 2025. [Online]. Disponível: https://dadosabertos.sp.gov.br
\bibitem{9} IBM, ``What is the k-nearest neighbors algorithm?,'' IBM Think Topics, 2025. [Online]. Disponível em: https://www.ibm.com/think/topics/knn
\bibitem{10} Fahs, W. et al., ``Traffic congestion prediction using machine learning techniques,'' \textit{Procedia Computer Science}, vol. 220, pp. 202--209, 2023.
\bibitem{11} Akhtar, M., \& Moridpour, S. (2021). A review of traffic congestion prediction using artificial intelligence. \textit{Journal of Advanced Transportation}, 2021, 1-18.
\bibitem{12} Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5--32.
\bibitem{13} Chen, T., \& Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 785--794). ACM.

\end{thebibliography}


\end{document}