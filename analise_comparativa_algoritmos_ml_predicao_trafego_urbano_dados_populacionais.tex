\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Análise Comparativa de Algoritmos de Aprendizado de Máquina na Predição do Crescimento do Trânsito Urbano a partir de Dados Populacionais\\
}

\author{\IEEEauthorblockN{Alexandre Marques Tortoza Canoa}
\IEEEauthorblockA{\textit{Escola Politécnica} \\
\textit{Pontifícia Universidade Católica do Paraná (PUCPR))}\\
Curitiba, PR, Brasil \\
a.marquestortoza@gmail.com}
}


\maketitle

\begin{abstract}
O agravamento do congestionamento urbano, impulsionado pelo crescimento populacional e pela expansão da frota de veículos, afeta diretamente a economia, o meio ambiente e o bem-estar social. Para estimar os níveis médios de tráfego nas regiões das cidades, foi aplicada uma abordagem preditiva que combina dados demográficos e históricos de trânsito. Após etapas de pré-processamento e codificação, foram testados diversos algoritmos de aprendizado supervisionado. Os modelos baseados em árvores apresentaram melhor desempenho, com destaque para o Random Forest. A população total foi a variável mais relevante, seguida por fatores temporais e espaciais. A integração entre dados populacionais e padrões de tráfego mostrou-se eficaz para apoiar o planejamento urbano e a gestão inteligente da mobilidade.
\end{abstract}

\begin{IEEEkeywords}
Predição de congestionamento, machine learning, dados populacionais, 
análise comparativa, random forest, tráfego urbano, série temporal, 
mobilidade urbana
\end{IEEEkeywords}

\section{Introdução}
O congestionamento urbano é um dos principais desafios enfrentados pelas grandes cidades contemporâneas. Intensificado pelo crescimento populacional acelerado e pela expansão desordenada da frota de veículos, esse fenômeno compromete diretamente a qualidade de vida, a eficiência econômica e a sustentabilidade ambiental \cite{1}. Estudos indicam que, em países desenvolvidos, os prejuízos causados pelo tráfego intenso somam bilhões de dólares por ano, refletindo em tempo perdido, aumento do consumo de combustível, elevação dos níveis de poluição e maior incidência de acidentes de trânsito. Na América Latina, o cenário é agravado pelo crescimento urbano desordenado e pela rápida motorização.

\subsection{Motivação}
Mitigar os impactos econômicos, sociais e ambientais do congestionamento é uma prioridade nas grandes metrópoles. A poluição, os atrasos nas viagens e os custos elevados de transporte exigem soluções eficazes e escaláveis. Antecipar cenários críticos de tráfego é essencial para otimizar a mobilidade e apoiar decisões em tempo real. Algoritmos de aprendizado de máquina desempenham papel significativo na análise de tráfego, permitindo identificar padrões complexos e prever situações de congestionamento \cite{2}. Quando aplicadas a dados históricos, essas técnicas viabilizam ações preventivas como ajustes semafóricos, orientação aos motoristas e melhorias no transporte público. Diversas abordagens têm demonstrado o potencial dessas técnicas para aumentar a eficiência do sistema viário.

\subsection{Problema}
Nas áreas urbanas, especialmente em grandes centros, o crescimento populacional e a motorização acelerada intensificam os congestionamentos, gerando impactos diretos na população e na economia. Entre os principais efeitos estão o aumento da poluição atmosférica e sonora, o maior consumo de combustível, a perda de tempo nas viagens e o crescimento das taxas de acidentes e infrações. Nos Estados Unidos, o congestionamento é apontado como uma ameaça ao desempenho econômico, enquanto na América Latina o problema se agrava com o crescimento desordenado das cidades.

\subsection{Objetivo}
Esta pesquisa busca prever, com antecedência, o nível de congestionamento em vias urbanas da cidade de São Paulo, utilizando séries temporais de volume de veículos. O objetivo é modelar a variação do fluxo ao longo do tempo, identificar padrões recorrentes e antecipar os momentos e locais mais pr   opensos a congestionamentos, oferecendo suporte ao planejamento e à gestão inteligente do trânsito.

\subsection{Artefato}
Será desenvolvido um sistema de aprendizado de máquina treinado com dados históricos de tráfego e rótulos de congestionamento. Após o treinamento, o modelo receberá dados atuais como entrada e estimará o nível de congestionamento de curto prazo, disponibilizando as previsões em uma interface voltada ao monitoramento e à tomada de decisão antecipada.

\section{Estado da Arte}
O congestionamento urbano permanece como um dos desafios mais persistentes das grandes metrópoles contemporâneas, intensificado pelo crescimento populacional acelerado e pela expansão desordenada das áreas urbanas. Em São Paulo, esse fenômeno atinge dimensões críticas, com a cidade frequentemente figurando entre as mais congestionadas do mundo e registrando perdas econômicas estimadas em bilhões de reais anualmente \cite{3}.

Tradicionalmente, as abordagens para mitigação do congestionamento concentravam-se em soluções infraestruturais de larga escala, como a ampliação da malha viária, estratégias predominantes nas décadas de 1950 e 1960 sob a influência do modelo rodoviarista \cite{1}. Contudo, a constatação de que a ampliação da oferta viária frequentemente induz à demanda adicional por deslocamentos motorizados levou à necessidade de repensar essas abordagens.

A partir dos anos 1990, o debate passou a incorporar dimensões sociais e de equidade urbana. Pereira e Schwanen \cite{4} demonstraram que trabalhadores de baixa renda, residentes em regiões periféricas, gastam até 20\% mais tempo em deslocamentos diários quando comparados a indivíduos de maior poder aquisitivo. Essa desigualdade espacial evidencia a necessidade de compreender não apenas os padrões gerais de tráfego, mas também sua relação com características demográficas e distribuição populacional.

O congestionamento resulta de uma combinação complexa de fatores recorrentes e não recorrentes. Entre os fatores recorrentes, destacam-se o volume excessivo de veículos, a capacidade limitada das vias e os horários de pico. Já os fatores não recorrentes incluem incidentes, obras e condições climáticas adversas \cite{2}. Essa multiplicidade de causas demanda ferramentas analíticas sofisticadas, capazes de processar grandes volumes de dados heterogêneos e identificar padrões complexos.

Com o advento das tecnologias digitais e a consolidação da era do Big Data, novas perspectivas metodológicas emergiram. A disponibilidade crescente de dados georreferenciados, séries temporais de tráfego e informações demográficas detalhadas viabilizou a aplicação de técnicas de aprendizado de máquina para previsão de condições de tráfego. Diferentemente das abordagens baseadas em modelos físicos e simulações de engenharia, que demandam parametrizações complexas, os modelos orientados por dados têm demonstrado capacidade superior de generalização e adaptação a padrões emergentes \cite{5}.

Estudos recentes têm explorado diferentes arquiteturas de aprendizado de máquina aplicadas à previsão de congestionamentos, demonstrando que modelos baseados em ensemble learning, como Random Forest e Gradient Boosting, superam técnicas lineares tradicionais em termos de precisão preditiva, especialmente quando confrontados com padrões não lineares \cite{6}. A capacidade desses modelos de capturar relações hierárquicas torna-os particularmente adequados para contextos urbanos caracterizados por heterogeneidade espacial e temporal.

A regressão linear, apesar de sua simplicidade, permanece relevante como baseline metodológico. Algoritmos baseados em instâncias, como K-Nearest Neighbors, têm sido aplicados devido à sua capacidade de capturar padrões locais, embora apresentem limitações de custo computacional em datasets volumosos. As técnicas de ensemble learning, representadas por Random Forest e XGBoost, têm alcançado desempenho superior em diversos problemas de previsão de tráfego, apresentando robustez contra overfitting e eficiência computacional em datasets de grande escala.

As redes neurais artificiais, em especial os Multilayer Perceptrons, constituem outra classe relevante de modelos. Embora exijam maiores volumes de dados para treinamento e apresentem menor interpretabilidade, oferecem flexibilidade arquitetural que permite modelar interações de alta ordem entre variáveis.

Um aspecto crítico na aplicação de técnicas de aprendizado de máquina é a capacidade de explicar as predições geradas. Técnicas de interpretabilidade como SHAP têm sido crescentemente adotadas para quantificar a contribuição individual de cada variável nas previsões de modelos complexos, permitindo que gestores urbanos compreendam quais fatores exercem maior influência sobre os padrões de congestionamento \cite{7}. Essa transparência metodológica é fundamental para a validação científica dos modelos e para a construção de confiança nas ferramentas computacionais.

A relação entre características populacionais e padrões de congestionamento constitui uma linha de investigação particularmente relevante, mas ainda pouco explorada. Enquanto estudos tradicionais concentram-se em variáveis diretas de tráfego, a incorporação de dados demográficos desagregados por região, faixa etária e gênero pode revelar padrões estruturais subjacentes que refletem a organização espacial das atividades urbanas.

No contexto específico de São Paulo, a heterogeneidade demográfica entre os diferentes distritos, combinada com a desigual distribuição de empregos formais e infraestrutura de transporte, sugere que modelos preditivos podem beneficiar-se significativamente da incorporação de variáveis populacionais. A modelagem conjunta dessas características demográficas com séries históricas de congestionamento permite não apenas prever condições futuras de tráfego, mas também identificar áreas prioritárias para intervenções de planejamento urbano.

Diante desse panorama, a presente pesquisa explora especificamente a capacidade de diferentes técnicas de aprendizado de máquina em prever padrões de congestionamento a partir de características populacionais desagregadas por distrito, sexo e faixa etária. A proposta metodológica enfatiza não apenas a comparação de desempenho preditivo entre os modelos, mas também a análise de interpretabilidade por meio de técnicas como SHAP, visando identificar quais variáveis demográficas exercem maior influência nos padrões de congestionamento observados na cidade de São Paulo.

\section{Metodologia}
A metodologia adotada nesta pesquisa foi estruturada em cinco etapas principais: aquisição de dados, pré-processamento, modelagem, avaliação e interpretação dos resultados. O pipeline seguido está ilustrado na Figura \ref{fig:pipeline}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.15\textwidth]{pipeline.png}
\caption{Pipeline da metodologia}
\label{fig:pipeline}
\end{figure}

Inicialmente, o estudo partiu de um dataset referente ao trânsito da cidade de São Paulo. Para enriquecer a análise e estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da capital, foram buscados dados complementares no Portal de Dados Abertos do Estado de São Paulo \cite{8}. Desse portal, foi extraído um segundo dataset contendo informações sobre a população de São Paulo ao longo dos anos.

Na segunda etapa, foi realizada a limpeza e padronização dos dados. Os datasets apresentavam ruídos, campos fora de padrão e diferentes codificações de texto (encodings), o que exigiu um processo cuidadoso de tratamento para garantir a compatibilidade e integridade das informações. Após a unificação dos dados, com as variáveis devidamente estruturadas, a pesquisa avançou para a etapa de modelagem com algoritmos de aprendizado de máquina.

Na terceira etapa, foram implementados e treinados diferentes modelos de machine learning, cada um com suas configurações e hiperparâmetros específicos. A quarta etapa consistiu na execução dos scripts desenvolvidos para obtenção dos valores das métricas de desempenho de cada modelo. Por fim, a quinta etapa envolveu a interpretação e análise comparativa dos resultados obtidos, permitindo avaliar a eficácia dos algoritmos frente aos dados tratados e compreender melhor a relação entre o crescimento populacional e o impacto no trânsito da cidade.

\subsection{Aquisição de Dados}\
O ponto de partida da pesquisa foi a seleção de um dataset sobre o trânsito da cidade de São Paulo. Este conjunto de dados se mostrou especialmente relevante por conter informações como dia, hora, região e, principalmente, o nível de congestionamento, variável central para os objetivos do estudo. O dataset original contém 77.159 registros com as seguintes variáveis: identificador único, data, hora, nome da via, direção, via expressa, tamanho do congestionamento em metros e região da cidade. Algumas colunas adicionais estavam presentes, mas foram descartadas por não contribuírem diretamente para a análise proposta.

Para estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da cidade, foi incorporado um segundo dataset, obtido por meio do Portal de Dados Abertos do Estado de São Paulo \cite{8}. Este conjunto (\texttt{estimativa\_pop\_idade\_sexo\_msp.csv}) trazia dados populacionais da capital paulista ao longo dos anos, incluindo as variáveis ano, código do distrito, nome do distrito, sexo, faixa etária e população total. O dataset original contém 81.601 registros distribuídos pelos distritos do município.

Apesar da riqueza informacional, esse segundo dataset apresentou desafios técnicos. Havia ruídos nos dados, campos fora de padrão e codificações de texto distintas (encodings), o que exigiu um esforço adicional para uniformização e tratamento adequado. Um ponto específico que merece destaque é a estrutura da variável idade, que estava agrupada em faixas etárias (por exemplo, "10 a 14 anos", "15 a 19 anos"). Essa segmentação dificultou a filtragem precisa de indivíduos com idade mínima para conduzir veículos (a partir dos 18 anos), sendo possível apenas considerar faixas a partir dos 20 anos, o que introduz um pequeno desvio na análise.

A aquisição dos dados, portanto, envolveu não apenas a seleção criteriosa dos conjuntos mais relevantes, mas também a antecipação dos desafios que seriam enfrentados nas etapas seguintes de tratamento e modelagem. A escolha destes datasets foi motivada pela relevância contextual, considerando que São Paulo enfrenta problemas crônicos de congestionamento com impactos econômicos e sociais significativos, pela confiabilidade das fontes oficiais com sistemas de monitoramento consolidados, e pela complementaridade entre dados de tráfego e demográficos, permitindo investigar a hipótese de relação entre crescimento populacional e padrões de congestionamento.

As variáveis de entrada (features) selecionadas foram: população total, hora do dia em formato numérico (0-23), via expressa codificada, região codificada, distribuição de sexo codificada, dia da semana (0-6) e mês (1-12). A variável de saída (target) definida foi o tamanho do congestionamento em metros.

\subsection{Limpeza e Preparação}

Para assegurar a consistência e a qualidade dos dados utilizados na modelagem preditiva, foram desenvolvidos dois pipelines automatizados de pré-processamento, um voltado ao tratamento dos dados populacionais e outro dedicado ao conjunto de dados de congestionamentos de tráfego. Ambos os processos tiveram como objetivo padronizar formatos, remover inconsistências e preparar os dados para a etapa de integração e posterior aplicação de algoritmos de aprendizado de máquina.

O conjunto populacional passou por um tratamento detalhado executado por script Python desenvolvido especificamente para este fim. Inicialmente, todos os campos textuais foram normalizados por meio da remoção de acentuação e da padronização de espaços, garantindo uniformidade entre distritos e categorias. Em seguida, foi aplicado um filtro para manter apenas as faixas etárias com idade igual ou superior a 20 anos, por conta de limitações do dataset, sendo esta a aproximação mais próxima da população apta a conduzir veículos. Na sequência, foi criada uma nova variável denominada região, obtida a partir do mapeamento de cada distrito para uma das cinco grandes áreas geográficas de São Paulo: norte, sul, leste, oeste e centro. Esse mapeamento foi implementado por meio de um dicionário abrangendo os 96 distritos oficiais do município, permitindo uma agregação coerente com as divisões espaciais utilizadas nos dados de tráfego. Por fim, distritos que não possuíam correspondência foram identificados e registrados em um arquivo auxiliar para verificação manual, garantindo a completude do mapeamento antes da exportação final dos dados, que foram salvos em formato CSV com codificação UTF-8 padronizada.

O conjunto de dados de tráfego, processado por script Python, exigiu um tratamento voltado principalmente à validação e à consistência das informações temporais e geográficas. As etapas iniciais incluíram a normalização dos campos textuais referentes a vias, regiões e expressways, removendo acentuação e espaços redundantes. Em seguida, foi realizada a validação das variáveis de data e hora, assegurando que apenas registros com formato temporal correto fossem mantidos. O campo de tamanho do congestionamento também passou por verificação, sendo eliminados valores negativos, nulos ou não numéricos, etapa considerada crítica pois a variável target precisa apresentar qualidade máxima para garantir a confiabilidade dos modelos. As regiões foram padronizadas para um formato em letras minúsculas e comparadas a uma lista de referência contendo as mesmas categorias usadas no dataset populacional (norte, sul, leste, oeste e centro), garantindo a compatibilidade entre as bases. Registros duplicados foram identificados e removidos através de comparação de chaves compostas (data, hora, via, região), e, ao final, foi gerado um relatório estatístico que resumiu a distribuição dos congestionamentos por região, além de medidas descritivas como média, mediana e desvio padrão do tamanho dos congestionamentos.

Ambos os conjuntos de dados resultantes foram padronizados quanto à codificação e ao delimitador, de forma a assegurar compatibilidade durante o processo de integração. A técnica de Label Encoding foi aplicada para transformar variáveis categóricas (região, via expressa, sexo) em valores numéricos ordinais, tornando-as adequadas para processamento pelos algoritmos de aprendizado de máquina. Os dados foram então agregados por data, hora, via e região para reduzir redundâncias e consolidar as informações. Registros com valores ausentes em features consideradas críticas foram removidos, priorizando a qualidade dos dados sobre a quantidade. Valores negativos de congestionamento foram removidos por representarem inconsistência física, enquanto outliers extremos foram identificados mas mantidos no dataset, pois podem representar situações reais de congestionamento severo que são relevantes para o modelo.

Essa padronização foi fundamental para que os datasets pudessem ser combinados por meio das variáveis região e ano, possibilitando análises conjuntas sobre o impacto da densidade populacional no comportamento do tráfego e permitindo que as etapas seguintes de modelagem preditiva fossem conduzidas de maneira consistente e reprodutível.

\subsection{Seleção de Modelos}
Foram selecionados cinco algoritmos de aprendizado de máquina supervisionado para avaliação comparativa, representando diferentes paradigmas de modelagem: Regressão Linear (modelo linear tradicional), K-Nearest Neighbors (modelo baseado em instâncias), Random Forest (ensemble de árvores de decisão), Multilayer Perceptron (rede neural artificial) e XGBoost (método de boosting baseado em árvores de decisão). Esta diversidade permite comparar abordagens com diferentes premissas teóricas, capacidades de modelagem e complexidades computacionais. Os algoritmos foram implementados utilizando a biblioteca scikit-learn para Python 3.13, e o ajuste de hiperparâmetros foi realizado através de experimentação empírica sistemática, priorizando configurações que apresentassem convergência estável e equilíbrio entre desempenho preditivo e custo computacional.

\subsection{Métricas de Avaliação}
Para avaliar o desempenho dos modelos, foram utilizadas três métricas apropriadas para problemas de regressão. O Mean Absolute Error (MAE), calculado como $\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$, onde $y_i$ representa o valor real, $\hat{y}_i$ o valor predito e $n$ o número de amostras, representa o erro médio absoluto em metros, sendo robusta a outliers e de interpretação direta. O Root Mean Squared Error (RMSE), definido como $\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$, também expressa o erro em metros, mas penaliza mais fortemente erros grandes devido ao termo quadrático, sendo que um RMSE significativamente maior que o MAE indica presença de outliers ou erros com alta variabilidade. O Coeficiente de Determinação ($R^2$), calculado como $R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$, onde $\bar{y}$ é a média dos valores reais, indica a proporção da variância da variável target que é explicada pelo modelo, variando entre 0 e 1 (podendo ser negativa para modelos inadequados), sendo que valores próximos a 1 indicam melhor capacidade preditiva.

\subsection{Avaliação dos Modelos}
Após a definição e implementação dos modelos de aprendizado de máquina, foi realizada a etapa de avaliação através da execução dos scripts desenvolvidos para cada algoritmo. Nesta etapa, os modelos foram treinados com os dados de treinamento e, em seguida, suas predições foram comparadas com os valores reais tanto no conjunto de treinamento quanto no conjunto de teste. Para cada modelo e configuração testada, foram registrados os valores das métricas MAE, RMSE e $R^2$, permitindo quantificar objetivamente o desempenho preditivo. Os resultados foram armazenados de forma estruturada para posterior análise comparativa. Esta etapa também incluiu a verificação de possíveis casos de overfitting, identificados quando o desempenho no conjunto de treinamento é substancialmente superior ao desempenho no conjunto de teste, indicando que o modelo memorizou os dados de treinamento ao invés de aprender padrões generalizáveis.



\begin{thebibliography}{00}

\bibitem{1} G. S. B. Vianna and C. Young, ``Em busca do tempo perdido: uma estimativa do produto perdido em trânsito no Brasil,'' \textit{Revista de Economia Contemporânea}, vol. 19, no. 3, pp. 403--416, set.--dez. 2015.
\bibitem{2} M. Akhtar and S. Moridpour, ``A review of traffic congestion prediction using artificial intelligence,'' \textit{Journal of Advanced Transportation}, vol. 2021, pp. 1--18, 2021.
\bibitem{3} D. B. Vale, ``The welfare costs of traffic congestion in São Paulo Metropolitan Area,'' Tese (Doutorado), Universidade de São Paulo, São Paulo, 2018.
\bibitem{4} R. H. M. Pereira and T. Schwanen, ``Tempo de deslocamento casa-trabalho no Brasil (1992-2009): diferenças entre regiões metropolitanas, níveis de renda e sexo,'' Texto para Discussão, IPEA, Brasília, n. 1813, 2013.
\bibitem{5} E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, ``Short-term traffic forecasting: where we are and where we're going,'' \textit{Transportation Research Part C: Emerging Technologies}, vol. 43, pp. 3--19, 2014.
\bibitem{6} P. Zechin et al., ``Traffic congestion prediction using machine learning techniques,'' in \textit{Proceedings of the 11th Brazilian Conference on Intelligent Systems}, Campinas: SBC, 2022, pp. 214--225.
\bibitem{7} S. M. Lundberg and S.-I. Lee, ``A unified approach to interpreting model predictions,'' in \textit{Proceedings of the 31st Conference on Neural Information Processing Systems}, Long Beach: NIPS, 2017, pp. 4765--4774.
\bibitem{8} Governo do Estado de São Paulo, ``Portal de Dados Abertos do Estado de São Paulo,'' 2025. [Online]. Disponível: https://dadosabertos.sp.gov.br

\end{thebibliography}


\end{document}
