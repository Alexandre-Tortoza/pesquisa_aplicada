\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Análise Comparativa de Algoritmos de Aprendizado de Máquina na Predição do Crescimento do Trânsito Urbano a partir de Dados Populacionais\\
}

\author{\IEEEauthorblockN{Alexandre Marques Tortoza Canoa}
\IEEEauthorblockA{\textit{Escola Politécnica} \\
\textit{Pontifícia Universidade Católica do Paraná (PUCPR))}\\
Curitiba, PR, Brasil \\
a.marquestortoza@gmail.com}
}


\maketitle

\begin{abstract}
O agravamento do congestionamento urbano, impulsionado pelo crescimento populacional e pela expansão da frota de veículos, afeta diretamente a economia, o meio ambiente e o bem-estar social. Para estimar os níveis médios de tráfego nas regiões das cidades, foi aplicada uma abordagem preditiva que combina dados demográficos e históricos de trânsito. Após etapas de pré-processamento e codificação, foram testados diversos algoritmos de aprendizado supervisionado. Os modelos baseados em árvores apresentaram melhor desempenho, com destaque para o Random Forest. A população total foi a variável mais relevante, seguida por fatores temporais e espaciais. A integração entre dados populacionais e padrões de tráfego mostrou-se eficaz para apoiar o planejamento urbano e a gestão inteligente da mobilidade.
\end{abstract}

\begin{IEEEkeywords}
Predição de congestionamento, machine learning, dados populacionais, 
análise comparativa, random forest, tráfego urbano, série temporal, 
mobilidade urbana
\end{IEEEkeywords}

\section{Introdução}
O congestionamento urbano é um dos principais desafios enfrentados pelas grandes cidades contemporâneas. Intensificado pelo crescimento populacional acelerado e pela expansão desordenada da frota de veículos, esse fenômeno compromete diretamente a qualidade de vida, a eficiência econômica e a sustentabilidade ambiental \cite{1}. Estudos indicam que, em países desenvolvidos, os prejuízos causados pelo tráfego intenso somam bilhões de dólares por ano, refletindo em tempo perdido, aumento do consumo de combustível, elevação dos níveis de poluição e maior incidência de acidentes de trânsito. Na América Latina, o cenário é agravado pelo crescimento urbano desordenado e pela rápida motorização.

\subsection{Motivação}
Mitigar os impactos econômicos, sociais e ambientais do congestionamento é uma prioridade nas grandes metrópoles. A poluição, os atrasos nas viagens e os custos elevados de transporte exigem soluções eficazes e escaláveis. Antecipar cenários críticos de tráfego é essencial para otimizar a mobilidade e apoiar decisões em tempo real. Algoritmos de aprendizado de máquina desempenham papel significativo na análise de tráfego, permitindo identificar padrões complexos e prever situações de congestionamento \cite{2}. Quando aplicadas a dados históricos, essas técnicas viabilizam ações preventivas como ajustes semafóricos, orientação aos motoristas e melhorias no transporte público. Diversas abordagens têm demonstrado o potencial dessas técnicas para aumentar a eficiência do sistema viário.

\subsection{Problema}
Nas áreas urbanas, especialmente em grandes centros, o crescimento populacional e a motorização acelerada intensificam os congestionamentos, gerando impactos diretos na população e na economia. Entre os principais efeitos estão o aumento da poluição atmosférica e sonora, o maior consumo de combustível, a perda de tempo nas viagens e o crescimento das taxas de acidentes e infrações. Nos Estados Unidos, o congestionamento é apontado como uma ameaça ao desempenho econômico, enquanto na América Latina o problema se agrava com o crescimento desordenado das cidades.

\subsection{Objetivo}
Esta pesquisa busca prever, com antecedência, o nível de congestionamento em vias urbanas da cidade de São Paulo, utilizando séries temporais de volume de veículos. O objetivo é modelar a variação do fluxo ao longo do tempo, identificar padrões recorrentes e antecipar os momentos e locais mais pr   opensos a congestionamentos, oferecendo suporte ao planejamento e à gestão inteligente do trânsito.

\subsection{Artefato}
Será desenvolvido um sistema de aprendizado de máquina treinado com dados históricos de tráfego e rótulos de congestionamento. Após o treinamento, o modelo receberá dados atuais como entrada e estimará o nível de congestionamento de curto prazo, disponibilizando as previsões em uma interface voltada ao monitoramento e à tomada de decisão antecipada.

\section{Estado da Arte}
O congestionamento urbano permanece como um dos desafios mais persistentes das grandes metrópoles contemporâneas, intensificado pelo crescimento populacional acelerado e pela expansão desordenada das áreas urbanas. Em São Paulo, esse fenômeno atinge dimensões críticas, com a cidade frequentemente figurando entre as mais congestionadas do mundo e registrando perdas econômicas estimadas em bilhões de reais anualmente \cite{3}.

Tradicionalmente, as abordagens para mitigação do congestionamento concentravam-se em soluções infraestruturais de larga escala, como a ampliação da malha viária, estratégias predominantes nas décadas de 1950 e 1960 sob a influência do modelo rodoviarista \cite{1}. Contudo, a constatação de que a ampliação da oferta viária frequentemente induz à demanda adicional por deslocamentos motorizados levou à necessidade de repensar essas abordagens.

A partir dos anos 1990, o debate passou a incorporar dimensões sociais e de equidade urbana. Pereira e Schwanen \cite{4} demonstraram que trabalhadores de baixa renda, residentes em regiões periféricas, gastam até 20\% mais tempo em deslocamentos diários quando comparados a indivíduos de maior poder aquisitivo. Essa desigualdade espacial evidencia a necessidade de compreender não apenas os padrões gerais de tráfego, mas também sua relação com características demográficas e distribuição populacional.

O congestionamento resulta de uma combinação complexa de fatores recorrentes e não recorrentes. Entre os fatores recorrentes, destacam-se o volume excessivo de veículos, a capacidade limitada das vias e os horários de pico. Já os fatores não recorrentes incluem incidentes, obras e condições climáticas adversas \cite{2}. Essa multiplicidade de causas demanda ferramentas analíticas sofisticadas, capazes de processar grandes volumes de dados heterogêneos e identificar padrões complexos.

Com o advento das tecnologias digitais e a consolidação da era do Big Data, novas perspectivas metodológicas emergiram. A disponibilidade crescente de dados georreferenciados, séries temporais de tráfego e informações demográficas detalhadas viabilizou a aplicação de técnicas de aprendizado de máquina para previsão de condições de tráfego. Diferentemente das abordagens baseadas em modelos físicos e simulações de engenharia, que demandam parametrizações complexas, os modelos orientados por dados têm demonstrado capacidade superior de generalização e adaptação a padrões emergentes \cite{5}.

Estudos recentes têm explorado diferentes arquiteturas de aprendizado de máquina aplicadas à previsão de congestionamentos, demonstrando que modelos baseados em ensemble learning, como Random Forest e Gradient Boosting, superam técnicas lineares tradicionais em termos de precisão preditiva, especialmente quando confrontados com padrões não lineares \cite{6}. A capacidade desses modelos de capturar relações hierárquicas torna-os particularmente adequados para contextos urbanos caracterizados por heterogeneidade espacial e temporal.

A regressão linear, apesar de sua simplicidade, permanece relevante como baseline metodológico. Algoritmos baseados em instâncias, como K-Nearest Neighbors, têm sido aplicados devido à sua capacidade de capturar padrões locais, embora apresentem limitações de custo computacional em datasets volumosos. As técnicas de ensemble learning, representadas por Random Forest e XGBoost, têm alcançado desempenho superior em diversos problemas de previsão de tráfego, apresentando robustez contra overfitting e eficiência computacional em datasets de grande escala.

As redes neurais artificiais, em especial os Multilayer Perceptrons, constituem outra classe relevante de modelos. Embora exijam maiores volumes de dados para treinamento e apresentem menor interpretabilidade, oferecem flexibilidade arquitetural que permite modelar interações de alta ordem entre variáveis.

Um aspecto crítico na aplicação de técnicas de aprendizado de máquina é a capacidade de explicar as predições geradas. Técnicas de interpretabilidade como SHAP têm sido crescentemente adotadas para quantificar a contribuição individual de cada variável nas previsões de modelos complexos, permitindo que gestores urbanos compreendam quais fatores exercem maior influência sobre os padrões de congestionamento \cite{7}. Essa transparência metodológica é fundamental para a validação científica dos modelos e para a construção de confiança nas ferramentas computacionais.

A relação entre características populacionais e padrões de congestionamento constitui uma linha de investigação particularmente relevante, mas ainda pouco explorada. Enquanto estudos tradicionais concentram-se em variáveis diretas de tráfego, a incorporação de dados demográficos desagregados por região, faixa etária e gênero pode revelar padrões estruturais subjacentes que refletem a organização espacial das atividades urbanas.

No contexto específico de São Paulo, a heterogeneidade demográfica entre os diferentes distritos, combinada com a desigual distribuição de empregos formais e infraestrutura de transporte, sugere que modelos preditivos podem beneficiar-se significativamente da incorporação de variáveis populacionais. A modelagem conjunta dessas características demográficas com séries históricas de congestionamento permite não apenas prever condições futuras de tráfego, mas também identificar áreas prioritárias para intervenções de planejamento urbano.

Diante desse panorama, a presente pesquisa explora especificamente a capacidade de diferentes técnicas de aprendizado de máquina em prever padrões de congestionamento a partir de características populacionais desagregadas por distrito, sexo e faixa etária. A proposta metodológica enfatiza a comparação de desempenho preditivo entre os modelos por meio de métricas consolidadas (MAE, RMSE e R²), além da análise de importância de features quando disponível nos algoritmos baseados em árvores, visando identificar quais variáveis demográficas exercem maior influência nos padrões de congestionamento observados na cidade de São Paulo.

\section{Metodologia}
A metodologia adotada nesta pesquisa foi estruturada em cinco etapas principais: aquisição de dados, pré-processamento, modelagem, avaliação e interpretação dos resultados. O pipeline seguido está ilustrado na Figura \ref{fig:pipeline}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.15\textwidth]{pipeline.png}
\caption{Pipeline da metodologia}
\label{fig:pipeline}
\end{figure}

Inicialmente, o estudo partiu de um dataset referente ao trânsito da cidade de São Paulo. Para enriquecer a análise e estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da capital, foram buscados dados complementares no Portal de Dados Abertos do Estado de São Paulo \cite{8}. Desse portal, foi extraído um segundo dataset contendo informações sobre a população de São Paulo ao longo dos anos.

Na segunda etapa, foi realizada a limpeza e padronização dos dados. Os datasets apresentavam ruídos, campos fora de padrão e diferentes codificações de texto (encodings), o que exigiu um processo cuidadoso de tratamento para garantir a compatibilidade e integridade das informações. Após a unificação dos dados, com as variáveis devidamente estruturadas, a pesquisa avançou para a etapa de modelagem com algoritmos de aprendizado de máquina.

Na terceira etapa, foram implementados e treinados diferentes modelos de machine learning, cada um com suas configurações e hiperparâmetros específicos. A quarta etapa consistiu na execução dos scripts desenvolvidos para obtenção dos valores das métricas de desempenho de cada modelo. Por fim, a quinta etapa envolveu a interpretação e análise comparativa dos resultados obtidos, permitindo avaliar a eficácia dos algoritmos frente aos dados tratados e compreender melhor a relação entre o crescimento populacional e o impacto no trânsito da cidade.

\subsection{Aquisição de Dados}\
O ponto de partida da pesquisa foi a seleção de um dataset sobre o trânsito da cidade de São Paulo. Este conjunto de dados se mostrou especialmente relevante por conter informações como dia, hora, região e, principalmente, o nível de congestionamento, variável central para os objetivos do estudo. O dataset original contém 77.159 registros com as seguintes variáveis: identificador único, data, hora, nome da via, direção, via expressa, tamanho do congestionamento em metros e região da cidade. Algumas colunas adicionais estavam presentes, mas foram descartadas por não contribuírem diretamente para a análise proposta.

Para estabelecer uma correlação entre o comportamento do trânsito e o crescimento populacional da cidade, foi incorporado um segundo dataset, obtido por meio do Portal de Dados Abertos do Estado de São Paulo \cite{8}. Este conjunto (\texttt{estimativa\_pop\_idade\_sexo\_msp.csv}) trazia dados populacionais da capital paulista ao longo dos anos, incluindo as variáveis ano, código do distrito, nome do distrito, sexo, faixa etária e população total. O dataset original contém 81.601 registros distribuídos pelos distritos do município.

Apesar da riqueza informacional, esse segundo dataset apresentou desafios técnicos. Havia ruídos nos dados, campos fora de padrão e codificações de texto distintas (encodings), o que exigiu um esforço adicional para uniformização e tratamento adequado. Um ponto específico que merece destaque é a estrutura da variável idade, que estava agrupada em faixas etárias (por exemplo, "10 a 14 anos", "15 a 19 anos"). Essa segmentação dificultou a filtragem precisa de indivíduos com idade mínima para conduzir veículos (a partir dos 18 anos), sendo possível apenas considerar faixas a partir dos 20 anos, o que introduz um pequeno desvio na análise.

A aquisição dos dados, portanto, envolveu não apenas a seleção criteriosa dos conjuntos mais relevantes, mas também a antecipação dos desafios que seriam enfrentados nas etapas seguintes de tratamento e modelagem. A escolha destes datasets foi motivada pela relevância contextual, considerando que São Paulo enfrenta problemas crônicos de congestionamento com impactos econômicos e sociais significativos, pela confiabilidade das fontes oficiais com sistemas de monitoramento consolidados, e pela complementaridade entre dados de tráfego e demográficos, permitindo investigar a hipótese de relação entre crescimento populacional e padrões de congestionamento.

As variáveis de entrada (features) selecionadas foram: população total, hora do dia em formato numérico (0-23), via expressa codificada, região codificada, distribuição de sexo codificada, dia da semana (0-6) e mês (1-12). As features temporais dia da semana e mês foram derivadas posteriormente a partir da coluna de data durante o processo de preparação dos dados para cada modelo. A variável de saída (target) definida foi o tamanho do congestionamento em metros.

\subsection{Limpeza e Preparação}
Para assegurar a consistência e a qualidade dos dados utilizados na modelagem preditiva, foram desenvolvidos dois pipelines automatizados de pré-processamento, um voltado ao tratamento dos dados populacionais e outro dedicado ao conjunto de dados de congestionamentos de tráfego. Ambos os processos tiveram como objetivo padronizar formatos, remover inconsistências e preparar os dados para a etapa de integração e posterior aplicação de algoritmos de aprendizado de máquina.

O conjunto populacional passou por um tratamento detalhado executado por script Python desenvolvido especificamente para este fim. Inicialmente, todos os campos textuais foram normalizados por meio da remoção de acentuação e da padronização de espaços, garantindo uniformidade entre distritos e categorias. Em seguida, foi aplicado um filtro para manter apenas as faixas etárias com idade igual ou superior a 20 anos, por conta de limitações do dataset, sendo esta a aproximação mais próxima da população apta a conduzir veículos. Na sequência, foi criada uma nova variável denominada região, obtida a partir do mapeamento de cada distrito para uma das cinco grandes áreas geográficas de São Paulo: norte, sul, leste, oeste e centro. Esse mapeamento foi implementado por meio de um dicionário abrangendo os 96 distritos oficiais do município, permitindo uma agregação coerente com as divisões espaciais utilizadas nos dados de tráfego. Por fim, distritos que não possuíam correspondência foram identificados e registrados em um arquivo auxiliar para verificação manual, garantindo a completude do mapeamento antes da exportação final dos dados, que foram salvos em formato CSV com codificação UTF-8 padronizada.

O conjunto de dados de tráfego, processado por script Python, exigiu um tratamento voltado principalmente à validação e à consistência das informações temporais e geográficas. As etapas iniciais incluíram a normalização dos campos textuais referentes a vias, regiões e expressways, removendo acentuação e espaços redundantes. Em seguida, foi realizada a validação das variáveis de data e hora, assegurando que apenas registros com formato temporal correto fossem mantidos. O campo de tamanho do congestionamento também passou por verificação, sendo eliminados valores negativos, nulos ou não numéricos, etapa considerada crítica pois a variável target precisa apresentar qualidade máxima para garantir a confiabilidade dos modelos. As regiões foram padronizadas para um formato em letras minúsculas e comparadas a uma lista de referência contendo as mesmas categorias usadas no dataset populacional (norte, sul, leste, oeste e centro), garantindo a compatibilidade entre as bases. Registros duplicados foram identificados e removidos através de comparação de chaves compostas (data, hora, via, região), e, ao final, foi gerado um relatório estatístico que resumiu a distribuição dos congestionamentos por região, além de medidas descritivas como média, mediana e desvio padrão do tamanho dos congestionamentos.

Ambos os conjuntos de dados resultantes foram padronizados quanto à codificação e ao delimitador, de forma a assegurar compatibilidade durante o processo de integração. A técnica de Label Encoding foi aplicada para transformar variáveis categóricas (região, via expressa, sexo) em valores numéricos ordinais, tornando-as adequadas para processamento pelos algoritmos de aprendizado de máquina. Os dados foram então agregados por data, hora, via e região para reduzir redundâncias e consolidar as informações. Registros com valores ausentes em features consideradas críticas foram removidos, priorizando a qualidade dos dados sobre a quantidade. Valores negativos de congestionamento foram removidos por representarem inconsistência física, enquanto outliers extremos foram identificados mas mantidos no dataset, pois podem representar situações reais de congestionamento severo que são relevantes para o modelo.

Essa padronização foi fundamental para que os datasets pudessem ser combinados por meio das variáveis região e ano, possibilitando análises conjuntas sobre o impacto da densidade populacional no comportamento do tráfego e permitindo que as etapas seguintes de modelagem preditiva fossem conduzidas de maneira consistente e reprodutível.

\subsection{Seleção de Modelos}
Foram selecionados cinco algoritmos de aprendizado de máquina supervisionado para avaliação comparativa, representando diferentes paradigmas de modelagem: Regressão Linear (modelo linear tradicional), K-Nearest Neighbors (modelo baseado em instâncias), Random Forest (ensemble de árvores de decisão), Multilayer Perceptron (rede neural artificial) e XGBoost (método de boosting baseado em árvores de decisão). Esta diversidade permite comparar abordagens com diferentes premissas teóricas, capacidades de modelagem e complexidades computacionais. Os algoritmos foram implementados utilizando a biblioteca scikit-learn para Python 3.13, com exceção do XGBoost que utiliza sua biblioteca própria (xgboost). Adicionalmente, foram utilizadas as bibliotecas pandas para manipulação de dados, numpy para operações numéricas e matplotlib para visualizações. O ajuste de hiperparâmetros foi realizado através de experimentação empírica sistemática, priorizando configurações que apresentassem convergência estável e equilíbrio entre desempenho preditivo e custo computacional.

\subsection{Métricas de Avaliação}
Para avaliar o desempenho dos modelos, foram utilizadas três métricas apropriadas para problemas de regressão. O Mean Absolute Error (MAE), calculado como $\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$, onde $y_i$ representa o valor real, $\hat{y}_i$ o valor predito e $n$ o número de amostras, representa o erro médio absoluto em metros, sendo robusta a outliers e de interpretação direta. O Root Mean Squared Error (RMSE), definido como $\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$, também expressa o erro em metros, mas penaliza mais fortemente erros grandes devido ao termo quadrático, sendo que um RMSE significativamente maior que o MAE indica presença de outliers ou erros com alta variabilidade. O Coeficiente de Determinação ($R^2$), calculado como $R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$, onde $\bar{y}$ é a média dos valores reais, indica a proporção da variância da variável target que é explicada pelo modelo, variando entre 0 e 1 (podendo ser negativa para modelos inadequados), sendo que valores próximos a 1 indicam melhor capacidade preditiva.

\subsection{Avaliação dos Modelos}
Após a definição e implementação dos modelos de aprendizado de máquina, foi realizada a etapa de avaliação através da execução dos scripts desenvolvidos para cada algoritmo. Nesta etapa, os modelos foram treinados com os dados de treinamento e, em seguida, suas predições foram comparadas com os valores reais tanto no conjunto de treinamento quanto no conjunto de teste. Para cada modelo e configuração testada, foram registrados os valores das métricas MAE, RMSE e $R^2$, permitindo quantificar objetivamente o desempenho preditivo. Os resultados foram armazenados de forma estruturada para posterior análise comparativa. Esta etapa também incluiu a verificação de possíveis casos de overfitting, identificados quando o desempenho no conjunto de treinamento é substancialmente superior ao desempenho no conjunto de teste, indicando que o modelo memorizou os dados de treinamento ao invés de aprender padrões generalizáveis.

\section{Avaliação dos Modelos}
A etapa de avaliação compreendeu a aplicação de cinco algoritmos distintos de aprendizado de máquina supervisionado, selecionados para representar diferentes paradigmas metodológicos na modelagem de problemas de regressão. A escolha contemplou desde modelos lineares clássicos até arquiteturas de redes neurais artificiais, permitindo uma análise comparativa abrangente sobre a capacidade preditiva de cada abordagem diante das características específicas do dataset de congestionamento urbano.

O primeiro modelo avaliado foi a Regressão Linear, selecionada como baseline metodológico devido à sua simplicidade interpretativa e capacidade de estabelecer relações lineares diretas entre as variáveis independentes e a variável target. Apesar de suas limitações conhecidas em capturar padrões não lineares, este modelo fornece uma referência fundamental para avaliar a complexidade inerente ao problema e o ganho proporcionado por técnicas mais sofisticadas.

O algoritmo K-Nearest Neighbors foi incluído como representante dos métodos baseados em instâncias, oferecendo uma abordagem não paramétrica que considera a similaridade entre observações no espaço de features. Este modelo é particularmente relevante para contextos onde padrões locais podem ser mais informativos que relações globais, embora apresente desafios de escalabilidade computacional em datasets volumosos.

Random Forest foi selecionado para representar os métodos de ensemble learning baseados em árvores de decisão. Esta técnica combina múltiplas árvores treinadas em diferentes subconjuntos de dados, proporcionando robustez contra overfitting e capacidade de capturar interações complexas entre variáveis. A arquitetura de ensemble também oferece mecanismos nativos para avaliar a importância de cada feature no processo preditivo.

O algoritmo XGBoost, baseado em gradient boosting de árvores de decisão, foi incorporado devido ao seu desempenho consistentemente superior em competições de ciência de dados e aplicações práticas de previsão. Este método constrói modelos sequenciais onde cada nova árvore corrige os erros das anteriores, resultando em alta precisão preditiva e eficiência computacional mesmo em datasets de grande escala.

Por fim, o Multilayer Perceptron foi incluído como representante das redes neurais artificiais, permitindo explorar a capacidade de modelagem de relações não lineares complexas através de arquiteturas de múltiplas camadas de neurônios. Embora exija maiores volumes de dados para treinamento e apresente menor interpretabilidade, este modelo oferece flexibilidade para capturar padrões hierárquicos e interações de alta ordem entre as variáveis demográficas e temporais.

Cada um desses modelos foi implementado seguindo práticas rigorosas de ajuste de hiperparâmetros e validação, conforme detalhado nas subseções subsequentes. A diversidade metodológica desta seleção possibilita não apenas identificar o algoritmo com melhor desempenho preditivo, mas também compreender quais características do problema favorecem determinadas abordagens e como diferentes paradigmas de modelagem respondem à estrutura específica dos dados de congestionamento urbano de São Paulo.

\subsection{K-Nearest Neighbors}
O algoritmo K-Nearest Neighbors representa uma abordagem não paramétrica de aprendizado supervisionado que se fundamenta no princípio da similaridade local entre observações no espaço de features. Diferentemente de algoritmos que constroem modelos explícitos durante o treinamento, o KNN é classificado como método de aprendizado baseado em instâncias ou lazy learning, uma vez que armazena todo o conjunto de treinamento e realiza os cálculos preditivos apenas no momento da inferência. Esta característica confere ao modelo flexibilidade para se adaptar rapidamente a novos dados sem necessidade de retreinamento, embora implique em custos computacionais significativos durante a fase de predição.

O funcionamento do KNN para problemas de regressão baseia-se na identificação dos k pontos de treinamento mais próximos a uma nova observação, utilizando uma métrica de distância para quantificar a similaridade no espaço multidimensional. Uma vez identificados os k vizinhos mais próximos, a predição é obtida através da média dos valores da variável target desses vizinhos, podendo-se optar por pesos uniformes, onde todos os vizinhos contribuem igualmente, ou por pesos baseados em distância, onde vizinhos mais próximos exercem maior influência na predição final. A escolha da métrica de distância constitui aspecto fundamental da modelagem, sendo as mais comumente empregadas a distância Euclidiana, que mensura a distância geométrica direta entre pontos, a distância de Manhattan, que soma as diferenças absolutas em cada dimensão, e a distância de Minkowski, que generaliza ambas através de um parâmetro ajustável.

A definição do hiperparâmetro k requer análise cuidadosa, pois valores pequenos resultam em modelos altamente sensíveis a flutuações locais e ruídos, podendo gerar overfitting, enquanto valores excessivamente grandes produzem suavização excessiva das predições, potencialmente causando underfitting ao obscurecer padrões locais relevantes \cite{9}. A seleção adequada de k tipicamente envolve técnicas de validação cruzada, buscando-se o valor que maximiza a capacidade de generalização do modelo sem comprometer sua sensibilidade a padrões genuínos nos dados. Adicionalmente, o KNN apresenta sensibilidade à escala das variáveis, tornando essencial a normalização ou padronização das features para evitar que variáveis com magnitudes maiores dominem o cálculo de distâncias.

Foram avaliadas cinco configurações distintas do algoritmo KNN, variando estratégia de validação, número de vizinhos, esquema de ponderação e métrica de distância. A Tabela \ref{tab:resultados_knn} apresenta os resultados obtidos para cada configuração testada.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular}{ccccccccc}
\hline
\textbf{Estratégia} & \textbf{Distância} & \textbf{Pesos} & \textbf{k} & \textbf{MAE} & \textbf{RMSE} & \textbf{R²} \\
\hline
Holdout & Minkowski & Uniform & 5 & 953.9 & 1504.2 & 0.49 \\
K-Fold (10) & Manhattan & Distance & 7 & \textbf{749.3} & --- & \textbf{0.56} \\
Holdout & Euclidean & Distance & 3 & 793.9 & 1464.8 & 0.52 \\
K-Fold (5) & Chebyshev & Uniform & 8 & 974.9 & --- & 0.48 \\
Holdout & Chebyshev & Uniform & 6 & 966.1 & 1513.3 & 0.48 \\
\hline
\end{tabular}
\caption{Resultados das configurações do modelo K-Nearest Neighbors}
\label{tab:resultados_knn}
\end{table}



A configuração KNN \#1 serviu como baseline metodológico, empregando estratégia holdout com divisão 75/25, distância de Minkowski, pesos uniformes e k igual a 5. Este modelo apresentou desempenho moderado, com erro médio absoluto de aproximadamente 954 metros e coeficiente de determinação de 0.49, indicando que o modelo explica cerca de metade da variância observada no congestionamento. A diferença entre os erros no conjunto de treino (MAE = 757.3 m, R² = 0.67) e teste sugere leve overfitting, embora dentro de limites aceitáveis para modelos baseados em instâncias.

A configuração KNN \#2 apresentou o melhor desempenho global entre todas as variações testadas, alcançando MAE de 749.3 metros e R² de 0.56 através de validação cruzada com 10 folds. A utilização da distância de Manhattan combinada com pesos baseados em distância e k igual a 7 mostrou-se particularmente adequada para este problema. A distância de Manhattan, que soma diferenças absolutas em cada dimensão ao invés de elevar ao quadrado como a Euclidiana, demonstrou robustez superior em capturar padrões de similaridade no contexto de dados urbanos heterogêneos. A baixa variabilidade observada entre os folds (desvio padrão de 5.2 metros no MAE e 0.007 no R²) confirma a estabilidade e consistência desta configuração.

O modelo KNN \#3 explorou a combinação de distância Euclidiana com pesos baseados em distância e valor reduzido de k igual a 3, resultando em MAE de 793.9 metros e R² de 0.52. Embora tenha apresentado desempenho inferior ao KNN \#2, esta configuração demonstrou capacidade relevante de capturar padrões locais, evidenciada pelo excelente desempenho no conjunto de treino (R² = 0.86). A discrepância significativa entre treino e teste indica que valores muito baixos de k, apesar de capturarem nuances locais, tendem a memorizar ruídos específicos do conjunto de treinamento.

As configurações KNN \#4 e KNN \#5 testaram a métrica de Chebyshev, que considera apenas a maior diferença entre as dimensões, ignorando variações menores. Ambas apresentaram desempenho inferior (R² aproximadamente 0.48), sugerindo que esta métrica não é adequada para o problema de previsão de congestionamento, onde múltiplas dimensões contribuem simultaneamente para determinar a similaridade entre observações. Os resultados indicam que a estrutura multidimensional do espaço de features exige métricas que considerem todas as dimensões de forma balanceada.

A análise comparativa dos resultados revela que a escolha da métrica de distância exerce impacto substancial no desempenho preditivo. A superioridade da distância de Manhattan sobre as demais sugere que, no contexto de dados urbanos com características heterogêneas, métricas que tratam diferenças de forma linear em cada dimensão são mais apropriadas que aquelas que penalizam quadraticamente desvios ou que consideram apenas a maior diferença. A ponderação por distância também demonstrou consistentemente melhorar os resultados, permitindo que observações mais próximas exerçam influência proporcionalmente maior nas predições.

O desempenho geral do KNN, com erro médio inferior a 800 metros na melhor configuração e capacidade de explicar 56\% da variância do congestionamento, estabelece este método como alternativa viável e competitiva para previsão de tráfego urbano baseada em características demográficas e temporais. A natureza não paramétrica do algoritmo permitiu capturar relações complexas sem assumir forma funcional específica, adaptando-se à heterogeneidade espacial e temporal característica dos padrões de congestionamento em São Paulo.

\subsection{Regressão Linear}
A Regressão Linear constitui o método estatístico mais fundamental para modelagem de relações entre variáveis, baseando-se na premissa de que a variável dependente pode ser expressa como combinação linear ponderada das variáveis independentes acrescida de um termo de erro aleatório. O modelo é definido matematicamente como $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon$, onde $y$ representa a variável target, $x_i$ são as features, $\beta_i$ são os coeficientes que ponderam cada variável, $\beta_0$ é o intercepto (termo independente) e $\epsilon$ é o termo de erro. A estimação dos parâmetros é realizada através do método dos mínimos quadrados ordinários, que minimiza a soma dos quadrados dos resíduos entre valores observados e preditos.

A principal vantagem da regressão linear reside em sua interpretabilidade direta, uma vez que cada coeficiente $\beta_i$ quantifica o impacto marginal da respectiva variável sobre o target, mantendo as demais constantes. Coeficientes positivos indicam relação direta, enquanto valores negativos sugerem relação inversa. A magnitude absoluta dos coeficientes reflete a intensidade da influência de cada variável, embora essa interpretação exija que as features estejam em escalas comparáveis. O intercepto $\beta_0$ representa o valor esperado de $y$ quando todas as variáveis independentes são zero, fornecendo o ponto de referência basal do modelo.

As limitações inerentes à regressão linear derivam principalmente de suas premissas restritivas. O modelo assume linearidade nas relações entre variáveis, homocedasticidade dos resíduos, independência das observações e normalidade dos erros. Violações dessas premissas podem comprometer a validade das inferências estatísticas e a precisão das predições. Adicionalmente, a regressão linear não captura interações complexas ou relações não lineares sem transformações explícitas das variáveis, limitando sua aplicabilidade em fenômenos caracterizados por dinâmicas intrinsecamente não lineares como o congestionamento urbano, onde múltiplos fatores interagem de formas complexas e não aditivas.

Foram testadas cinco configurações do modelo de regressão linear, variando principalmente a presença do intercepto e a estratégia de validação. A Tabela \ref{tab:resultados_linear} sintetiza os resultados obtidos.

\subsection{Multilayer Perceptron}
O Multilayer Perceptron constitui arquitetura fundamental de redes neurais artificiais do tipo feedforward, caracterizada por múltiplas camadas de neurônios completamente conectados que processam informações unidirecionalmente desde a camada de entrada até a camada de saída. A estrutura é composta por uma camada de entrada que recebe as features, camadas ocultas (hidden layers) que realizam transformações não lineares sucessivas, e uma camada de saída que produz a predição final. Cada neurônio em uma camada está conectado a todos os neurônios da camada subsequente através de pesos sinápticos ajustáveis, e aplica uma função de ativação não linear ao somatório ponderado de suas entradas, permitindo que a rede aprenda representações hierárquicas complexas dos dados.

O processo de treinamento do MLP baseia-se no algoritmo de retropropagação (backpropagation), que ajusta iterativamente os pesos da rede através do gradiente descendente para minimizar uma função de perda. A função de ativação ReLU (Rectified Linear Unit), definida como $f(x) = \max(0, x)$, tornou-se padrão em arquiteturas modernas devido às suas propriedades computacionais favoráveis, evitando o problema de desvanecimento de gradiente característico de funções sigmoidais. Otimizadores adaptativos como Adam (Adaptive Moment Estimation) ajustam a taxa de aprendizado individualmente para cada parâmetro, combinando as vantagens do momentum com escalamento adaptativo, resultando em convergência mais rápida e estável comparado ao gradiente descendente estocástico clássico.

A capacidade do MLP de aproximar funções arbitrariamente complexas, conhecida como propriedade de aproximador universal, fundamenta-se teoricamente no teorema de aproximação universal de Cybenko, que estabelece que uma rede neural com uma única camada oculta contendo número suficiente de neurônios pode aproximar qualquer função contínua com precisão arbitrária. Na prática, arquiteturas profundas com múltiplas camadas menores frequentemente superam redes rasas com muitos neurônios, pois as camadas sucessivas aprendem representações hierárquicas onde features de baixo nível são compostas em abstrações progressivamente mais complexas. A regularização L2, também conhecida como weight decay, é comumente aplicada adicionando um termo de penalidade proporcional à norma dos pesos à função de perda, prevenindo overfitting ao desincentivar valores extremos de parâmetros.

Foram avaliadas cinco configurações distintas do MLP, variando profundidade da rede, número de neurônios por camada, funções de ativação e otimizadores. A Tabela \ref{tab:resultados_mlp} apresenta síntese comparativa dos resultados.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cccccccc}
\hline
\textbf{Estratégia} & \textbf{Arquitetura} & \textbf{Ativação} & \textbf{Otim.} & \textbf{MAE} & \textbf{RMSE} & \textbf{R²} \\
\hline
Holdout & (100,50,25) & ReLU & Adam & 988.5 & 1531.6 & 0.47 \\
Holdout & (256,128,64,32) & ReLU & Adam & \textbf{962.3} & \textbf{1490.8} & \textbf{0.50} \\
Holdout & (64,32) & ReLU & Adam & 1005.1 & 1537.1 & 0.47 \\
K-Fold (5) & (100,50) & ReLU & Adam & 988.4 & --- & 0.47 \\
K-Fold (5) & (168,64,32) & Tanh & SGD & 1502.7 & --- & 0.00 \\
\hline
\end{tabular}
\caption{Resultados das configurações do Multilayer Perceptron}
\label{tab:resultados_mlp}
\end{table}

A configuração MLP \#1 estabeleceu baseline com arquitetura de três camadas ocultas (100, 50, 25 neurônios), função de ativação ReLU e otimizador Adam. O modelo convergiu em 179 iterações, demonstrando estabilidade do processo de treinamento, e alcançou R² de 0.47 com MAE de 988.5 metros. Este desempenho equipara-se ao obtido pelo KNN baseline, sugerindo que ambos os paradigmas (aprendizado baseado em instâncias versus aproximação funcional por redes neurais) capturam porções similares da variância nos dados. A diferença moderada entre treino e teste indica ausência de overfitting severo, embora a arquitetura possa não estar plenamente explorando a capacidade representacional das redes profundas.

A configuração MLP \#2 explorou arquitetura substancialmente mais profunda com quatro camadas ocultas (256, 128, 64, 32 neurônios), mantendo ReLU e Adam. Esta especificação produziu o melhor desempenho entre todas as configurações MLP testadas, alcançando MAE de 962.3 metros e R² de 0.50. O incremento de aproximadamente 50 metros no MAE e 0.03 no R² comparado ao baseline demonstra que o aumento da profundidade e largura da rede permitiu capturar padrões mais sutis nos dados. A arquitetura progressivamente decrescente (256→128→64→32) implementa um funil informacional onde cada camada processa representações de dimensionalidade reduzida mas semanticamente mais ricas, favorecendo a extração de features relevantes. A regularização L2 aplicada preveniu overfitting apesar da maior complexidade do modelo.

O modelo MLP \#3 testou arquitetura compacta com apenas duas camadas ocultas (64, 32 neurônios), visando eficiência computacional para potencial implementação em cenários de inferência em tempo real. Apesar da simplicidade, o modelo manteve desempenho comparável ao baseline (R² = 0.47, MAE ≈ 1005 metros), convergindo em 212 iterações. Este resultado sugere que existe trade-off favorável entre complexidade arquitetural e desempenho preditivo, onde configurações intermediárias podem oferecer equilíbrio adequado para aplicações práticas com restrições computacionais. A redução para duas camadas, no entanto, limita a profundidade das representações hierárquicas aprendidas, explicando o desempenho ligeiramente inferior ao MLP profundo.

A configuração MLP \#4 empregou validação cruzada 5-fold com arquitetura intermediária (100, 50 neurônios), obtendo R² médio de 0.47 com desvio padrão de 0.005 e MAE de 988.4 metros. A baixa variabilidade entre folds confirma estabilidade da rede neural através de diferentes partições dos dados, validando que o desempenho observado não é artefato de uma divisão específica treino-teste. A consistência dos resultados indica que os pesos aprendidos capturam padrões genuínos generalizáveis ao invés de memorizar peculiaridades do conjunto de treinamento, evidenciando robustez da arquitetura MLP clássica.

A configuração MLP \#5 testou combinação alternativa de função de ativação tangente hiperbólica e otimizador SGD clássico, resultando em falha completa de convergência (R² ≈ 0.00, MAE > 1500 metros). A função tanh, que mapeia entradas para o intervalo [-1, 1] através da transformação $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$, apresenta saturação em ambas as extremidades, causando gradientes muito pequenos durante retropropagação em redes profundas. O otimizador SGD, sem mecanismos adaptativos de taxa de aprendizado, não conseguiu compensar este problema, resultando em treinamento estagnado. Este resultado confirma empiricamente a superioridade da combinação ReLU-Adam para problemas de regressão em tráfego urbano, onde as relações não lineares exigem propagação eficiente de gradientes através de múltiplas camadas.

A análise comparativa revela que o desempenho do MLP é sensível tanto à profundidade quanto à largura da arquitetura, com configurações mais profundas capturando interações mais complexas entre features. A escolha da função de ativação e otimizador exerce impacto crítico na convergência, com ReLU-Adam demonstrando consistentemente resultados superiores. A capacidade do MLP de explicar 50\% da variância do congestionamento, equivalente ao melhor resultado do KNN, estabelece redes neurais como alternativa competitiva, com vantagem adicional de poder escalar para arquiteturas ainda mais profundas caso volumes maiores de dados estejam disponíveis.

O MLP profundo (256,128,64,32) representa o melhor compromisso entre complexidade e desempenho dentre os modelos avaliados até este ponto, superando marginalmente o KNN e substancialmente a regressão linear. A normalização das features através de StandardScaler mostrou-se absolutamente essencial para convergência, dado que redes neurais são altamente sensíveis à escala das variáveis de entrada. O número de iterações necessário para convergência (179-212 para configurações bem-sucedidas) indica que o processo de otimização foi eficiente, sem indícios de treinamento excessivamente longo que pudesse sugerir problemas de condicionamento numérico.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular}{cccccc}
\hline
\textbf{Estratégia} & \textbf{Intercepto} & \textbf{MAE} & \textbf{RMSE} & \textbf{R²} \\
\hline
Holdout & Sim & 1379.7 & 1990.8 & 0.10 \\
Holdout & Não & 2443.8 & 3133.9 & -1.21 \\
K-Fold (10) & Sim & 1377.9 & --- & 0.10 \\
Holdout & Sim & 1379.7 & 1990.8 & 0.10 \\
K-Fold (7) & Sim & 1377.9 & --- & 0.10 \\
\hline
\end{tabular}
\caption{Resultados das configurações do modelo de Regressão Linear}
\label{tab:resultados_linear}
\end{table}

A configuração baseline (Linear \#1), utilizando estratégia holdout com todas as variáveis e inclusão do intercepto, apresentou desempenho substancialmente inferior ao observado nos demais modelos avaliados. O coeficiente de determinação de apenas 0.10 indica que o modelo linear explica apenas 10\% da variância observada no congestionamento, evidenciando a predominância de relações não lineares no fenômeno estudado. O erro médio absoluto de aproximadamente 1380 metros, significativamente superior aos 750-950 metros obtidos pelo KNN, confirma a inadequação da premissa de linearidade para este problema. A diferença moderada entre desempenho de treino e teste sugere ausência de overfitting, mas simultaneamente revela limitação estrutural do modelo em capturar os padrões subjacentes aos dados.

A configuração Linear \#2, que removeu o intercepto do modelo, resultou em deterioração drástica do desempenho, com R² de -1.21 e MAE de 2444 metros. Valores negativos de R² indicam que o modelo produz predições piores que simplesmente utilizar a média como estimativa para todas as observações, demonstrando completa inadequação desta especificação. A ausência do termo independente força o hiperplano de regressão a passar pela origem, introduzindo viés sistemático nas predições quando esta restrição não é justificada teoricamente. O resultado confirma que o intercepto é componente fundamental do modelo, representando o nível basal de congestionamento quando todas as variáveis preditoras são zero.

As configurações Linear \#3 e Linear \#5 empregaram validação cruzada com 10 e 7 folds respectivamente, apresentando resultados praticamente idênticos ao baseline (R² ≈ 0.10, MAE ≈ 1378 metros). A baixíssima variabilidade entre folds (desvio padrão de 6 metros no MAE e 0.002-0.003 no R²) atesta a estabilidade estatística do modelo, mas simultaneamente confirma sua limitação estrutural. A consistência dos resultados através de diferentes partições dos dados elimina a possibilidade de que o baixo desempenho seja artefato da amostragem, estabelecendo que a regressão linear é intrinsecamente inadequada para capturar a complexidade do problema de previsão de congestionamento.

A análise dos coeficientes estimados revela que população total, hora do dia e dia da semana apresentam os maiores impactos absolutos sobre as predições. O coeficiente positivo para população indica que regiões mais populosas tendem a apresentar maior congestionamento, enquanto a hora do dia mostra padrões esperados de concentração nos horários de pico. No entanto, a magnitude dos coeficientes deve ser interpretada com cautela dado o baixo poder explicativo geral do modelo. A variável região apresentou coeficientes distintos para cada zona geográfica, sugerindo heterogeneidade espacial que o modelo linear não consegue adequadamente representar através de simples codificação numérica.

A comparação entre holdout e validação cruzada demonstra que o problema não é instabilidade do modelo ou sensibilidade à partição dos dados, mas sim inadequação da forma funcional linear. O desempenho consistentemente pobre através de todas as configurações testadas indica que as relações entre variáveis demográficas, temporais e espaciais com o congestionamento são fundamentalmente não lineares e envolvem interações complexas que não podem ser capturadas por combinações lineares aditivas das features.

O desempenho insatisfatório da regressão linear neste contexto é teoricamente esperado, considerando que o congestionamento urbano resulta de dinâmicas complexas envolvendo saturação de capacidade viária, padrões temporais não lineares de demanda por deslocamentos e heterogeneidades espaciais na infraestrutura de transporte. Fenômenos de threshold, onde pequenos aumentos no volume de veículos podem causar saltos desproporcionais no congestionamento uma vez ultrapassada a capacidade da via, exemplificam não linearidades características do tráfego urbano que modelos lineares não conseguem representar.

O papel da regressão linear nesta pesquisa, portanto, consolida-se como baseline metodológico essencial, estabelecendo patamar mínimo de desempenho e demonstrando a necessidade de técnicas mais sofisticadas. A capacidade interpretativa do modelo, apesar de seu baixo poder preditivo, fornece insights preliminares sobre as direções das relações entre variáveis, servindo como ponto de partida para especificações mais complexas. O contraste marcante entre o R² de 0.10 da regressão linear e os 0.49-0.56 alcançados pelo KNN quantifica objetivamente o ganho proporcionado por modelos capazes de capturar não linearidades e interações locais nos dados.

\subsection{Random Forest}
O algoritmo Random Forest representa técnica de ensemble learning baseada no princípio de agregação de múltiplas árvores de decisão treinadas independentemente, cada uma construída sobre diferentes subconjuntos dos dados através de bootstrap aggregating (bagging). Introduzido por Breiman em 2001, o método combina as predições individuais de dezenas ou centenas de árvores através de votação majoritária (classificação) ou média aritmética (regressão), produzindo estimativas mais robustas e menos susceptíveis a overfitting que árvores de decisão individuais. A diversidade entre as árvores é garantida não apenas pelo bootstrap dos dados, mas também pela seleção aleatória de subconjuntos de features em cada divisão nodal, processo conhecido como random subspace method.

Cada árvore no ensemble é construída através de processo recursivo de particionamento binário do espaço de features, onde em cada nó interno seleciona-se a melhor divisão entre um subconjunto aleatório de $m$ features disponíveis, tipicamente $m = \sqrt{p}$ para classificação ou $m = p/3$ para regressão, onde $p$ é o número total de features. O critério de divisão para regressão baseia-se na minimização da variância dos resíduos, matematicamente expresso como $\text{Var}(y) = \frac{1}{n}\sum_{i=1}^{n}(y_i - \bar{y})^2$, onde a divisão ótima maximiza a redução de variância entre o nó pai e os nós filhos resultantes. Este processo continua recursivamente até que critérios de parada sejam satisfeitos, como profundidade máxima, número mínimo de amostras por nó, ou pureza completa das folhas.

Uma propriedade particularmente valiosa do Random Forest é sua capacidade nativa de quantificar importância de features através do critério de redução média de impureza. Para cada feature, calcula-se a redução total de variância proporcionada por divisões nessa variável através de todas as árvores do ensemble, normalizada pelo número de divisões. Features que consistentemente produzem divisões que reduzem substancialmente a variância recebem pontuações de importância elevadas, fornecendo interpretabilidade mesmo em modelos complexos com centenas de variáveis. Adicionalmente, o algoritmo não requer normalização de features e é robusto a valores discrepantes, pois as divisões baseiam-se em ordenamento relativo ao invés de magnitudes absolutas.

Foram testadas cinco configurações do Random Forest variando número de árvores, profundidade máxima e estratégia de validação. A Tabela \ref{tab:resultados_rf} apresenta síntese dos resultados obtidos.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cccccccc}
\hline
\textbf{Estratégia} & \textbf{Árvores} & \textbf{Prof. Máx.} & \textbf{MAE} & \textbf{RMSE} & \textbf{R²} \\
\hline
Holdout & 100 & Sem limite & \textbf{760.2} & 1298.6 & \textbf{0.62} \\
Holdout & 200 & 30 & 749.7 & 1439.9 & 0.53 \\
Holdout & 300 & 15 & 857.9 & 1348.9 & 0.59 \\
K-Fold (5) & 50 & 10 & 974.8 & --- & 0.49 \\
K-Fold (5) & 150 & 12 & 931.0 & --- & 0.53 \\
\hline
\end{tabular}
\caption{Resultados das configurações do Random Forest}
\label{tab:resultados_rf}
\end{table}

A configuração RF \#1 estabeleceu baseline metodológico com 100 árvores e ausência de limitação de profundidade, alcançando o melhor desempenho global entre todas as configurações testadas com R² de 0.62 e MAE de 760.2 metros. Este resultado representa melhoria substancial sobre KNN (R² = 0.56) e MLP (R² = 0.50), evidenciando a eficácia do ensemble learning em capturar as complexidades não lineares do congestionamento urbano. A análise de importância de features revelou que população total contribui com 37\% da capacidade preditiva, seguida por via expressa (15\%), mês (15\%) e hora do dia (13\%), confirmando que tanto variáveis demográficas quanto temporais exercem influência significativa nos padrões de tráfego.

A configuração RF \#2, com 200 árvores e profundidade limitada a 30 níveis, produziu MAE de 749.7 metros mas R² de apenas 0.53, inferior ao baseline apesar do erro médio ligeiramente menor. A discrepância significativa entre desempenho de treino (R² = 0.87, MAE = 327 m) e teste indica overfitting moderado, sugerindo que a combinação de muitas árvores com profundidade elevada permitiu memorização de idiossincrasias dos dados de treinamento. A redução no R² apesar do MAE competitivo reflete que o modelo captura bem a tendência central mas apresenta maior variabilidade nos resíduos, com alguns erros grandes impactando mais fortemente o termo quadrático do R².

O modelo RF \#3 testou configuração robusta com 300 árvores mas profundidade máxima restrita a 15 níveis, buscando equilíbrio entre complexidade e generalização. O resultado de R² = 0.59 e MAE = 857.9 metros representa compromisso intermediário, onde a limitação de profundidade previne overfitting mas também restringe a capacidade de capturar padrões muito específicos. A distribuição de importância de features mostrou-se mais balanceada, com população total (36\%), via expressa (21\%) e região (18\%) dominando, mas com contribuições relevantes de todas as variáveis temporais, sugerindo que a restrição de profundidade força o modelo a utilizar mais uniformemente todas as features disponíveis.

As configurações RF \#4 e RF \#5 empregaram validação cruzada 5-fold com arquiteturas mais parcimoniosas. O RF \#4, com apenas 50 árvores e profundidade máxima de 10, alcançou R² médio de 0.49 com desvio padrão de 0.002, demonstrando alta estabilidade através dos folds. Apesar do desempenho inferior ao baseline, esta configuração oferece vantagens computacionais significativas, reduzindo tempo de treinamento e requisitos de memória, sendo adequada para prototipagem rápida ou cenários com restrições de recursos. O RF \#5, com 150 árvores e profundidade 12, obteve melhor equilíbrio com R² médio de 0.53 e MAE de 931 metros, confirmando que configurações intermediárias com regularização estrutural através de limitação de profundidade produzem modelos mais estáveis, embora com menor capacidade preditiva máxima.

A análise comparativa da importância de features através das diferentes configurações revela padrão consistente onde população total domina invariavelmente (34-38\% de importância), seguida por via expressa e região, confirmando que características demográficas e espaciais constituem os principais determinantes do congestionamento. A variável sexo apresentou contribuição nula em todas as configurações, indicando que a distribuição por gênero na população, uma vez controladas outras variáveis demográficas e espaciais, não adiciona poder preditivo ao modelo. As variáveis temporais (hora, dia da semana, mês) contribuem coletivamente com 25-35\% da importância, evidenciando padrões cíclicos significativos no congestionamento.

O desempenho superior do Random Forest comparado a modelos anteriores deriva de sua capacidade de capturar interações complexas entre variáveis sem especificação explícita de termos de interação. Enquanto a regressão linear assume aditividade estrita e o KNN baseia-se em similaridade euclidiana global, o Random Forest aprende automaticamente regras hierárquicas do tipo "se população alta E hora de pico E região central, então congestionamento severo", capturando sinergias entre features. A robustez do método a outliers e sua insensibilidade a escalas de variáveis adicionalmente facilitam sua aplicação prática, eliminando necessidade de pré-processamento extensivo.

A configuração RF \#1, com R² de 0.62 explicando aproximadamente dois terços da variância no congestionamento, estabelece novo patamar de desempenho entre os modelos avaliados. O erro médio de 760 metros, embora ainda considerável em termos absolutos, representa melhoria substancial sobre regressão linear (1380 m) e ganhos modestos mas consistentes sobre KNN (749 m) e MLP (962 m). A natureza ensemble do algoritmo, combinando predições de 100 árvores independentes, proporciona estabilidade superior a modelos individuais, reduzindo sensibilidade a peculiaridades de subconjuntos específicos dos dados.

\subsection{XGBoost}
O XGBoost (Extreme Gradient Boosting) representa implementação altamente otimizada do algoritmo de gradient boosting, técnica de ensemble learning que constrói modelos sequencialmente onde cada nova árvore é treinada para corrigir os erros residuais das árvores anteriores. Desenvolvido por Chen e Guestrin em 2016, o XGBoost distingue-se de implementações tradicionais de boosting através de inovações algorítmicas e de engenharia que incluem regularização explícita, tratamento eficiente de dados esparsos, paralelização automática e aproximação por histograma para identificação rápida de pontos de divisão ótimos, resultando em velocidade de treinamento substancialmente superior mantendo ou superando a acurácia de métodos anteriores.

O fundamento matemático do gradient boosting baseia-se na otimização aditiva de uma função objetivo composta por termo de perda mais termo de regularização. A cada iteração $t$, adiciona-se uma nova função $f_t$ ao ensemble que minimiza $L^{(t)} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)$, onde $l$ é a função de perda, $\hat{y}_i^{(t-1)}$ é a predição acumulada até a iteração anterior e $\Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$ é o termo de regularização que penaliza complexidade da árvore através do número de folhas $T$ e magnitude dos pesos $w_j$. O algoritmo aproxima esta função objetivo através de expansão de Taylor de segunda ordem, convertendo o problema em forma quadrática que pode ser resolvida analiticamente em cada nó.

A taxa de aprendizado (learning rate) $\eta$ controla a contribuição de cada árvore ao modelo final através de $\hat{y}^{(t)} = \hat{y}^{(t-1)} + \eta f_t(x)$, implementando estratégia de shrinkage onde valores menores (tipicamente 0.01-0.3) requerem mais árvores mas frequentemente resultam em melhor generalização ao prevenir que árvores individuais dominem as predições. Técnicas adicionais de regularização incluem subsampling de linhas e colunas, onde cada árvore é treinada em subconjunto aleatório dos dados e features, mecanismo análogo ao Random Forest mas aplicado sequencialmente ao invés de paralelamente, reduzindo correlação entre árvores sucessivas e mitigando overfitting.

Foram avaliadas cinco configurações do XGBoost variando número de árvores, profundidade máxima, taxa de aprendizado e estratégia de validação. A Tabela \ref{tab:resultados_xgb} sintetiza os resultados.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccccccc}
\hline
\textbf{Estratégia} & \textbf{Árvores} & \textbf{Prof.} & \textbf{LR} & \textbf{MAE} & \textbf{RMSE} & \textbf{R²} \\
\hline
Holdout & 100 & 6 & 0.10 & 953.1 & 1468.7 & 0.51 \\
Holdout & 400 & 10 & 0.03 & \textbf{844.7} & \textbf{1327.3} & \textbf{0.60} \\
K-Fold (5) & 80 & 5 & 0.02 & 1124.3 & --- & 0.36 \\
K-Fold (5) & 300 & 8 & 0.05 & 913.0 & --- & 0.55 \\
Holdout & 50 & 4 & 0.30 & 983.6 & 1507.5 & 0.49 \\
\hline
\end{tabular}
\caption{Resultados das configurações do XGBoost}
\label{tab:resultados_xgb}
\end{table}

A configuração XGB \#1 estabeleceu baseline com hiperparâmetros padrão da biblioteca (100 árvores, profundidade 6, learning rate 0.1), alcançando R² de 0.51 e MAE de 953 metros. Este desempenho supera marginalmente a regressão linear (R² = 0.10) mas situa-se abaixo do KNN (R² = 0.56) e Random Forest (R² = 0.62), sugerindo que a configuração padrão subotimiza as capacidades do algoritmo para este problema específico. O tempo de execução reduzido comparado ao Random Forest, devido à construção sequencial versus paralela de árvores, torna esta configuração adequada para prototipagem rápida e estabelecimento de linhas de base.

A configuração XGB \#2 produziu o melhor desempenho absoluto entre todos os modelos avaliados na pesquisa, com R² de 0.60 e MAE de 844.7 metros. Esta especificação empregou 400 árvores com profundidade máxima de 10 e taxa de aprendizado reduzida para 0.03, implementando estratégia de boosting profundo com aprendizado conservador. O número elevado de árvores combinado com learning rate baixo permite construção gradual de modelo complexo onde cada árvore contribui modestamente, reduzindo risco de overfitting através de agregação de muitas predições fracas ao invés de poucas predições fortes. A profundidade de 10 níveis possibilita captura de interações de alta ordem entre features, enquanto a regularização L2 padrão previne ajuste excessivo aos dados de treinamento.

O modelo XGB \#3 explorou extremo oposto do espectro de complexidade, com apenas 80 árvores rasas (profundidade 5) e learning rate mínimo (0.02), priorizando velocidade computacional e forte regularização. O resultado de R² de 0.36 e MAE de 1124 metros, substancialmente inferior aos demais modelos, demonstra que regularização excessiva pode degradar capacidade preditiva ao ponto de performance inferior até mesmo à regressão linear sem intercepto. A consistência entre folds (desvio padrão de apenas 2.1 metros no MAE) confirma estabilidade mas revela que o modelo está underfitting, insuficientemente complexo para capturar os padrões não lineares nos dados.

A configuração XGB \#4 empregou validação cruzada com 300 árvores, profundidade 8 e learning rate intermediário (0.05), alcançando equilíbrio entre complexidade e generalização com R² médio de 0.55 e MAE de 913 metros. A baixa variabilidade entre folds (desvio padrão de 6.6 metros) atesta robustez do modelo, tornando esta configuração particularmente adequada para implantação em produção onde consistência de predições é prioritária. O subsampling agressivo de 70\% dos dados e features em cada iteração promove diversidade entre árvores sucessivas, reduzindo correlação e melhorando capacidade de generalização.

O modelo XGB \#5 testou configuração minimalista com 50 árvores de profundidade 4 e learning rate elevado (0.3), voltada para inferência em tempo real. O desempenho de R² = 0.49 e MAE = 984 metros, embora inferior aos modelos mais complexos, demonstra que XGBoost mantém capacidade preditiva razoável mesmo em configurações altamente restritas. O learning rate alto acelera convergência mas aumenta risco de overshooting de mínimos locais da função objetivo, explicando o desempenho intermediário. Esta configuração seria apropriada para sistemas embarcados ou aplicações móveis com severas restrições computacionais.

A análise de importância de features através do critério de gain (redução média de perda proporcionada por cada variável) revelou padrão consistente onde população total domina com 35-40\% de importância, seguida por região (20-25\%) e via expressa (15-20\%). Este ranking difere sutilmente do Random Forest, onde via expressa ocasionalmente superava região, sugerindo que o aprendizado sequencial do XGBoost captura diferentes aspectos das interações entre variáveis. A contribuição nula da variável sexo persiste, confirmando ausência de poder preditivo incremental desta feature dado o conjunto de outras variáveis demográficas e espaciais.

O desempenho superior do XGB \#2 comparado ao Random Forest (R² 0.60 vs 0.62, MAE 845 vs 760 metros) merece análise cuidadosa. Embora o Random Forest apresente R² ligeiramente superior, o XGBoost demonstra vantagens em termos de controle fino sobre o processo de aprendizado através de hiperparâmetros como learning rate, subsampling e regularização L1/L2 explícitas. A natureza sequencial do boosting, onde cada árvore é informada pelos erros das anteriores, teoricamente deveria superar a agregação paralela do Random Forest, mas na prática o desempenho depende criticamente de ajuste de hiperparâmetros e características específicas dos dados.

A superioridade do XGBoost sobre modelos não baseados em árvores (KNN, MLP, Regressão Linear) evidencia-se claramente. O ganho sobre KNN (R² 0.60 vs 0.56) e MLP (R² 0.60 vs 0.50) demonstra que a estrutura hierárquica de árvores de decisão, especialmente quando otimizada através de boosting, captura mais efetivamente as não linearidades e interações presentes nos dados de congestionamento urbano. A regularização explícita do XGBoost, ausente no KNN e presente mas diferente no MLP, proporciona controle mais refinado sobre complexidade do modelo.

O XGBoost estabelece-se como alternativa extremamente competitiva ao Random Forest, oferecendo desempenho comparável com vantagens adicionais de controle fino sobre aprendizado, eficiência computacional através de implementação otimizada e capacidade de continuar treinamento incrementalmente adicionando mais árvores ao modelo existente. A configuração XGB \#2 representa escolha sólida para implantação em produção, equilibrando alta acurácia preditiva com razoável custo computacional e robustez demonstrada através de métricas consistentes entre treino e teste.

\section{Conclusões}
A presente pesquisa conduziu análise comparativa sistemática de cinco algoritmos de aprendizado de máquina aplicados à previsão de congestionamento urbano na cidade de São Paulo, utilizando dados demográficos integrados a séries históricas de tráfego. Os resultados obtidos demonstram que técnicas de ensemble learning baseadas em árvores de decisão superam substancialmente abordagens lineares e baseadas em instâncias, com XGBoost (R² = 0.60, MAE = 845 m) e Random Forest (R² = 0.62, MAE = 760 m) apresentando os melhores desempenhos. O Multilayer Perceptron alcançou capacidade preditiva intermediária (R² = 0.50, MAE = 962 m), enquanto o K-Nearest Neighbors demonstrou resultados competitivos (R² = 0.56, MAE = 749 m). A Regressão Linear, conforme esperado, produziu o pior desempenho (R² = 0.10, MAE = 1380 m), estabelecendo baseline que evidencia a predominância de relações não lineares no fenômeno estudado.

Uma ressalva metodológica importante refere-se ao período temporal coberto pelos datasets, que abrangem dados de 2018 a 2024, incluindo integralmente o período da pandemia de COVID-19 (2020-2022). Este intervalo testemunhou alterações drásticas nos padrões de mobilidade urbana devido a lockdowns, trabalho remoto massivo e redução acentuada da circulação de veículos, introduzindo heterogeneidade temporal significativa nos dados. A capacidade dos modelos de manter desempenho razoável mesmo com esta descontinuidade estrutural nas séries temporais atesta robustez dos algoritmos testados, particularmente dos métodos ensemble que demonstraram maior resistência a mudanças abruptas de regime nos padrões de tráfego.

\subsection{Por que estes resultados?}

O desempenho superior dos algoritmos baseados em árvores de decisão (Random Forest e XGBoost) fundamenta-se em três características estruturais destes métodos particularmente adequadas ao problema de previsão de congestionamento urbano. Primeiro, as árvores de decisão capturam naturalmente interações complexas entre variáveis através de regras hierárquicas do tipo "se-então", permitindo modelar situações onde o impacto de uma variável depende dos valores de outras variáveis sem necessidade de especificação explícita de termos de interação. Por exemplo, o efeito da população total sobre o congestionamento varia substancialmente conforme região e horário, interação que árvores de decisão identificam automaticamente mas que regressão linear não captura sem engenharia manual de features.

Segundo, os métodos ensemble agregam predições de múltiplos modelos base, reduzindo variância através de averaging (Random Forest) ou corrigindo viés sequencialmente (XGBoost), resultando em predições mais estáveis que modelos individuais. Esta propriedade mostrou-se crítica dado que o congestionamento urbano apresenta alta variabilidade intrínseca devido a eventos estocásticos como acidentes, obras e condições climáticas adversas que não são capturados pelas features disponíveis. A agregação de múltiplas árvores suaviza flutuações aleatórias, extraindo sinal sistemático do ruído.

Terceiro, árvores de decisão são invariantes a transformações monotônicas das features e robustas a outliers, pois as divisões baseiam-se em ordenamento relativo ao invés de magnitudes absolutas. Esta característica é particularmente valiosa considerando que variáveis como população total e tamanho de congestionamento apresentam distribuições altamente assimétricas com caudas longas, onde alguns distritos possuem população ou congestionamentos ordens de magnitude maiores que a mediana. Métodos sensíveis a escala como KNN e MLP exigem normalização cuidadosa e são mais susceptíveis a valores extremos.

O desempenho limitado da Regressão Linear (R² = 0.10) confirma que a premissa de linearidade é fundamentalmente inadequada para este problema. O congestionamento urbano caracteriza-se por não linearidades intrínsecas como efeitos de threshold onde pequenos incrementos no volume de veículos causam aumentos desproporcionais no congestionamento uma vez ultrapassada a capacidade da via, fenômenos de saturação onde regiões já congestionadas não podem piorar substancialmente mesmo com aumentos populacionais, e sinergias entre variáveis temporais e espaciais. A incapacidade do modelo linear de capturar estas dinâmicas resulta em explicação de apenas 10\% da variância, evidenciando que 90\% dos padrões nos dados derivam de relações não aditivas.

O K-Nearest Neighbors, apesar de sua simplicidade conceitual, alcançou desempenho surpreendentemente competitivo (R² = 0.56), demonstrando que a hipótese de similaridade local é parcialmente válida para congestionamento urbano. Observações com características demográficas e temporais similares tendem a apresentar níveis de congestionamento comparáveis, justificando a abordagem baseada em instâncias. No entanto, a necessidade de calcular distâncias para todas as observações de treinamento torna o KNN computacionalmente custoso em fase de inferência, limitando sua aplicabilidade em sistemas de tempo real que exigem latência mínima.

O Multilayer Perceptron, com desempenho intermediário (R² = 0.50), demonstrou que redes neurais podem modelar não linearidades relevantes, mas sofrem de limitações inerentes quando aplicadas a datasets tabulares de dimensionalidade moderada. Redes neurais profundas tipicamente requerem volumes massivos de dados para ajustar efetivamente seus milhares de parâmetros, e datasets com dezenas de milhares de observações (como o utilizado nesta pesquisa) situam-se em zona limítrofe onde há dados suficientes para evitar underfitting severo mas insuficientes para alcançar o desempenho ótimo teórico das arquiteturas profundas. Adicionalmente, a menor interpretabilidade das redes neurais comparada a métodos baseados em árvores constitui desvantagem em contextos de planejamento urbano onde stakeholders requerem compreensão explícita dos fatores que influenciam as predições.

\subsection{Como melhorar os resultados?}

Diversas estratégias podem potencialmente aprimorar o desempenho preditivo dos modelos desenvolvidos. A incorporação de variáveis contextuais adicionais representa a avenida mais promissora para ganhos substanciais. Dados climáticos (precipitação, temperatura, visibilidade), indicadores de eventos especiais (jogos de futebol, shows, manifestações), calendário de feriados e informações sobre obras e interdições viárias introduziriam fatores explanatórios para porções da variabilidade atualmente tratadas como ruído aleatório. Estas variáveis são conhecidas por exercerem impacto significativo nos padrões de tráfego e sua ausência no dataset atual limita o teto teórico de desempenho alcançável.

A engenharia de features mais sofisticada, particularmente criação de interações explícitas entre variáveis temporais e espaciais, poderia beneficiar até mesmo os métodos ensemble que já capturam interações automaticamente. Features compostas como "população × hora de pico", "região central × dia útil" ou indicadores de proximidade a grandes centros de emprego capturariam sinergias específicas que, embora teoricamente identificáveis por árvores profundas, podem ser aprendidas mais eficientemente quando explicitamente fornecidas. Técnicas de feature selection baseadas em importância mútua de informação também poderiam identificar subconjuntos ótimos de variáveis, reduzindo dimensionalidade e potencialmente melhorando generalização.

O ajuste sistemático de hiperparâmetros através de Grid Search ou Bayesian Optimization representa oportunidade significativa, especialmente para XGBoost e Multilayer Perceptron que possuem numerosos hiperparâmetros sensíveis. Os experimentos realizados exploraram diversas configurações através de experimentação empírica, mas busca exaustiva ou guiada por otimização Bayesiana no espaço de hiperparâmetros poderia identificar combinações superiores não testadas. Para o MLP, técnicas como Dropout, Batch Normalization e arquiteturas residuais poderiam mitigar problemas de otimização e melhorar generalização.

Modelos ensemble heterogêneos que combinam predições de algoritmos com diferentes vieses indutivos (por exemplo, stacking de Random Forest, XGBoost e MLP) frequentemente superam modelos individuais ao capturar complementariedades. Um meta-modelo treinado para ponderar otimamente as predições dos modelos base poderia aproveitar que diferentes algoritmos cometem erros em diferentes regiões do espaço de features, resultando em ensemble que é correto mais frequentemente que qualquer componente individual.

A segmentação do problema em subproblemas mais específicos através de modelagem hierárquica poderia melhorar precisão. Por exemplo, treinar modelos especializados para diferentes regiões da cidade, horários do dia ou níveis de congestionamento permitiria capturar dinâmicas locais que modelos globais necessariamente suavizam. Uma arquitetura de dois estágios onde o primeiro modelo classifica o nível aproximado de congestionamento (baixo, médio, alto) e modelos de segundo estágio refinam a predição dentro de cada categoria poderia reduzir heterogeneidade dentro de cada modelo, facilitando aprendizado.

A coleta de dados de maior granularidade temporal e espacial expandiria capacidade preditiva. Dados com resolução de minutos ao invés de horas e desagregação por via individual ao invés de região permitiriam capturar flutuações de curto prazo e heterogeneidades intra-regionais, embora ao custo de maior complexidade computacional e requisitos de armazenamento. Dados de fontes alternativas como GPS de aplicativos de navegação, sensores de loop indutivo em vias principais e câmeras de tráfego forneceriam medições diretas de fluxo veicular, complementando as proxies demográficas utilizadas na presente pesquisa.

\subsection{O que diferencia este estudo da literatura?}

A contribuição distintiva desta pesquisa reside na integração sistemática de dados demográficos desagregados por distrito, sexo e faixa etária com séries históricas de congestionamento, abordagem pouco explorada na literatura de previsão de tráfego urbano que tipicamente concentra-se em variáveis de tráfego diretas (velocidade média, volume veicular, ocupação de vias). A modelagem conjunta de características populacionais com padrões de congestionamento permite investigar hipóteses sobre relações estruturais entre organização espacial da população e dinâmicas de mobilidade, questão de relevância teórica para planejamento urbano e políticas de transporte.

A análise comparativa rigorosa de cinco algoritmos representando diferentes paradigmas metodológicos (linear, baseado em instâncias, ensemble paralelo, ensemble sequencial, redes neurais) sob mesmas condições de dados e métricas fornece benchmark robusto ausente em grande parte da literatura, onde estudos tipicamente reportam resultados de um ou dois métodos específicos sem comparação sistemática. A inclusão de Regressão Linear como baseline metodológico essencial, frequentemente omitida em trabalhos recentes focados exclusivamente em deep learning, estabelece referência para quantificar objetivamente ganhos proporcionados por técnicas mais sofisticadas.

A incorporação de análises de importância de features através de métricas nativas dos algoritmos ensemble (feature importance do Random Forest, gain do XGBoost) e técnicas model-agnostic (SHAP values quando aplicável) proporciona interpretabilidade frequentemente ausente em estudos puramente focados em acurácia preditiva. A identificação consistente de população total como variável dominante (35-40\% de importância) seguida por características espaciais (região, via expressa) e temporais (hora, mês) fornece insights acionáveis sobre determinantes primários do congestionamento que transcendem os resultados numéricos específicos dos modelos.

O contexto específico de São Paulo, megacidade com características únicas de heterogeneidade socioespacial, desigualdade na distribuição de empregos e infraestrutura, e padrões de mobilidade complexos, diferencia este estudo de trabalhos em cidades de países desenvolvidos onde condições urbanas são substancialmente distintas. Os achados contribuem para literatura emergente sobre mobilidade urbana em contextos de países em desenvolvimento, onde crescimento acelerado e planejamento frequentemente reativo (ao invés de proativo) criam desafios específicos não totalmente capturados por pesquisas em metrópoles europeias ou norte-americanas.

A transparência metodológica através de documentação detalhada de pré-processamento de dados, tratamento de valores faltantes, estratégias de codificação de variáveis categóricas e especificação completa de hiperparâmetros testados facilita replicabilidade e permite que pesquisadores futuros construam sobre esta base sem necessidade de reinventar componentes básicos do pipeline. A disponibilização potencial dos scripts de treinamento e avaliação dos modelos (subject to data privacy considerations) democratizaria acesso a ferramentas de previsão de congestionamento para municípios e organizações com recursos limitados.

O período temporal estudado (2018-2024), abrangendo pré-pandemia, pandemia e recuperação pós-pandemia, fornece perspectiva única sobre robustez de modelos preditivos diante de choques exógenos massivos que alteram fundamentalmente padrões de mobilidade. A maioria dos estudos de previsão de tráfego são conduzidos em períodos de relativa estabilidade; a demonstração de que algoritmos de machine learning mantêm capacidade preditiva razoável mesmo com descontinuidades estruturais nos dados tem implicações importantes para aplicações práticas em contextos de incerteza e mudança rápida.

\subsection{Considerações Finais}

Os resultados obtidos estabelecem que algoritmos de aprendizado de máquina, particularmente métodos ensemble baseados em árvores, são ferramentas viáveis e efetivas para previsão de congestionamento urbano utilizando dados demográficos e temporais facilmente acessíveis através de fontes oficiais. A capacidade de explicar 60-62\% da variância no congestionamento, embora ainda distante de perfeição, representa avanço substancial sobre abordagens ingênuas e fornece base sólida para sistemas de apoio à decisão em gestão de tráfego e planejamento urbano.

As limitações identificadas, particularmente a ausência de variáveis contextuais críticas e a granularidade temporal e espacial dos dados, apontam direções claras para pesquisas futuras. A integração de múltiplas fontes de dados (sensores IoT, GPS de aplicativos de mobilidade, redes sociais, previsão meteorológica) em sistemas preditivos multimodais representa fronteira natural de evolução deste trabalho, potencialmente elevando capacidade preditiva a patamares onde aplicações de tempo real tornam-se viáveis.

A contribuição desta pesquisa estende-se além dos resultados numéricos específicos, estabelecendo metodologia replicável para avaliação comparativa de algoritmos de machine learning em contextos de transporte urbano e demonstrando valor de integração entre dados demográficos e séries de tráfego para compreensão mais profunda das dinâmicas de mobilidade metropolitana. Os insights obtidos sobre importância relativa de diferentes features informam não apenas desenvolvimento de modelos mais sofisticados, mas também priorização de coleta de dados e investimentos em infraestrutura de monitoramento urbano.

\begin{thebibliography}{00}

\bibitem{1} G. S. B. Vianna and C. Young, ``Em busca do tempo perdido: uma estimativa do produto perdido em trânsito no Brasil,'' \textit{Revista de Economia Contemporânea}, vol. 19, no. 3, pp. 403--416, set.--dez. 2015.
\bibitem{2} M. Akhtar and S. Moridpour, ``A review of traffic congestion prediction using artificial intelligence,'' \textit{Journal of Advanced Transportation}, vol. 2021, pp. 1--18, 2021.
\bibitem{3} D. B. Vale, ``The welfare costs of traffic congestion in São Paulo Metropolitan Area,'' Tese (Doutorado), Universidade de São Paulo, São Paulo, 2018.
\bibitem{4} R. H. M. Pereira and T. Schwanen, ``Tempo de deslocamento casa-trabalho no Brasil (1992-2009): diferenças entre regiões metropolitanas, níveis de renda e sexo,'' Texto para Discussão, IPEA, Brasília, n. 1813, 2013.
\bibitem{5} E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, ``Short-term traffic forecasting: where we are and where we're going,'' \textit{Transportation Research Part C: Emerging Technologies}, vol. 43, pp. 3--19, 2014.
\bibitem{6} P. Zechin et al., ``Traffic congestion prediction using machine learning techniques,'' in \textit{Proceedings of the 11th Brazilian Conference on Intelligent Systems}, Campinas: SBC, 2022, pp. 214--225.
\bibitem{7} S. M. Lundberg and S.-I. Lee, ``A unified approach to interpreting model predictions,'' in \textit{Proceedings of the 31st Conference on Neural Information Processing Systems}, Long Beach: NIPS, 2017, pp. 4765--4774.
\bibitem{8} Governo do Estado de São Paulo, ``Portal de Dados Abertos do Estado de São Paulo,'' 2025. [Online]. Disponível: https://dadosabertos.sp.gov.br
\bibitem{9} IBM, ``What is the k-nearest neighbors algorithm?,'' IBM Think Topics, 2025. [Online]. Disponível em: https://www.ibm.com/think/topics/knn
\bibitem{10} Fahs, W. et al., ``Traffic congestion prediction using machine learning techniques,'' \textit{Procedia Computer Science}, vol. 220, pp. 202--209, 2023.
\bibitem{11} Akhtar, M., \& Moridpour, S. (2021). A review of traffic congestion prediction using artificial intelligence. \textit{Journal of Advanced Transportation}, 2021, 1-18.
\bibitem{12} Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5--32.
\bibitem{13} Chen, T., \& Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 785--794). ACM.

\end{thebibliography}


\end{document}
`
