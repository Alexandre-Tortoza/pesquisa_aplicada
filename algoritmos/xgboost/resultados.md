# AnÃ¡lise de Resultados Modelo XGBoost

## Significado das MÃ©tricas

| Sigla       | Nome Completo                   | DescriÃ§Ã£o                                                                                                                       |
| ----------- | ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| **MAE**     | _Mean Absolute Error_           | Erro mÃ©dio absoluto â€” mede, em metros, o quanto as previsÃµes diferem dos valores reais. Valores menores indicam maior precisÃ£o. |
| **RMSE**    | _Root Mean Squared Error_       | Raiz do erro quadrÃ¡tico mÃ©dio â€” penaliza mais fortemente erros grandes. Ideal para avaliar estabilidade do modelo.              |
| **RÂ²**      | _Coeficiente de DeterminaÃ§Ã£o_   | Mede o quanto o modelo explica a variÃ¢ncia dos dados reais. Varia entre 0 e 1; quanto maior, melhor o ajuste.                   |
| **Holdout** | DivisÃ£o simples de treino/teste | Parte dos dados Ã© usada para treino e outra para teste, permitindo avaliaÃ§Ã£o direta da generalizaÃ§Ã£o.                           |
| **K-Fold**  | ValidaÃ§Ã£o Cruzada               | Divide os dados em _k_ partes (folds), treinando e testando vÃ¡rias vezes. Reduz viÃ©s e aumenta robustez da avaliaÃ§Ã£o.           |

---

## ConfiguraÃ§Ãµes Avaliadas

Foram testadas **cinco variaÃ§Ãµes** do **XGBoost Regressor**, explorando combinaÃ§Ãµes de profundidade, nÃºmero de Ã¡rvores, taxa de aprendizado e mÃ©todo de validaÃ§Ã£o.

| Modelo     | EstratÃ©gia | Ãrvores | Profundidade | Learning Rate | MAE Teste | RMSE Teste | RÂ² Teste | ObservaÃ§Ãµes                                       |
| ---------- | ---------- | ------- | ------------ | ------------- | --------- | ---------- | -------- | ------------------------------------------------- |
| **XGB #1** | Holdout    | 100     | 6            | 0.10          | 953.1     | 1468.7     | 0.51     | Modelo base com hiperparÃ¢metros padrÃ£o            |
| **XGB #2** | Holdout    | 400     | 10           | 0.03          | **844.7** | **1327.3** | **0.60** | Profundo e estÃ¡vel, melhor resultado geral        |
| **XGB #3** | K-Fold (5) | 80      | 5            | 0.02          | 1124.3    | â€”          | 0.36     | Simples e regularizado, prioriza velocidade       |
| **XGB #4** | K-Fold (5) | 300     | 8            | 0.05          | **913.0** | â€”          | **0.55** | Alta estabilidade e generalizaÃ§Ã£o                 |
| **XGB #5** | Holdout    | 50      | 4            | 0.30          | 983.6     | 1507.5     | 0.49     | Leve e rÃ¡pido, voltado Ã  inferÃªncia em tempo real |

---

## AnÃ¡lise dos Resultados

### âš™ï¸ XGB #1 â€” Baseline (100 Ã¡rvores, profundidade 6)

Modelo padrÃ£o com configuraÃ§Ã£o balanceada.
AlcanÃ§ou **RÂ² = 0.51** e **MAE â‰ˆ 953 m**, servindo como linha de base.
Mostra bom desempenho geral e baixo viÃ©s, com tempo de execuÃ§Ã£o reduzido.
Ideal para prototipagem e comparaÃ§Ãµes.

---

### ğŸŒ³ XGB #2 â€” Profundo e Preciso (400 Ã¡rvores, max_depth=10)

O melhor modelo entre todos.
Com **RÂ² = 0.60** e **MAE â‰ˆ 845 m**, capturou **relaÃ§Ãµes nÃ£o-lineares complexas** entre variÃ¡veis regionais, temporais e populacionais.
A taxa de aprendizado reduzida (0.03) e o nÃºmero maior de Ã¡rvores suavizaram o aprendizado, evitando overfitting.
Excelente equilÃ­brio entre desempenho e estabilidade â€” **modelo mais indicado para produÃ§Ã£o.**

---

### ğŸŒ± XGB #3 â€” Regularizado e Leve (80 Ã¡rvores, max_depth=5)

VersÃ£o mais simples, priorizando eficiÃªncia e regularizaÃ§Ã£o L1/L2.
Obteve **RÂ² = 0.36** e **MAE â‰ˆ 1124 m**, com alta estabilidade entre folds.
Apesar do desempenho inferior, mostrou-se Ãºtil como **baseline otimizado para execuÃ§Ã£o rÃ¡pida**, ideal para comparaÃ§Ã£o de tuning.

---

### ğŸŒ² XGB #4 â€” Alta Estabilidade (300 Ã¡rvores, max_depth=8)

Usando validaÃ§Ã£o cruzada e subsampling agressivo, o modelo atingiu **RÂ² mÃ©dio = 0.55 Â± 0.005** e **MAE â‰ˆ 913 m**.
Mostra **consistÃªncia entre folds** e resistÃªncia a ruÃ­dos.
Indicado para aplicaÃ§Ãµes em larga escala, onde estabilidade e previsibilidade sÃ£o prioritÃ¡rias.

---

### ğŸƒ XGB #5 â€” Modelo RÃ¡pido (50 Ã¡rvores, max_depth=4)

Focado em **tempo de inferÃªncia**, o modelo alcanÃ§ou **RÂ² = 0.49**, com erro mÃ©dio de **984 m**.
Desempenho competitivo considerando o custo computacional mÃ­nimo.
Adequado para cenÃ¡rios de atualizaÃ§Ã£o em tempo real ou dispositivos com restriÃ§Ã£o de hardware.

---

## ConclusÃµes Gerais

- ğŸ† **Melhor desempenho geral:** XGB #2 (400 Ã¡rvores, profundidade 10, LR 0.03) â€” **RÂ² = 0.60, MAE = 845 m**
  â†’ Modelo ideal para implantaÃ§Ã£o, equilibrando precisÃ£o e generalizaÃ§Ã£o.

- âš™ï¸ **Vantagem do XGBoost:** Combina interpretabilidade das Ã¡rvores com poder de modelagem nÃ£o-linear, **superando RF e MLP** em consistÃªncia geral.

- ğŸ§© **VariÃ¡veis mais relevantes (padrÃ£o entre execuÃ§Ãµes):**
  `pop_total`, `regiao_encoded` e `via_expressa_encoded` dominam a explicaÃ§Ã£o da variÃ¢ncia.

- ğŸš¦ O XGBoost mostrou **melhor estabilidade e menor viÃ©s** que o Random Forest, alÃ©m de convergÃªncia mais previsÃ­vel.

---

## PrÃ³ximos Passos

1. ğŸ”§ Realizar _fine-tuning_ de `learning_rate`, `subsample` e `colsample_bytree` via _Grid Search_.
2. ğŸ“ˆ Avaliar impacto de features adicionais como clima e feriados.
3. ğŸ§® Explorar tÃ©cnicas de _feature interaction_ para relaÃ§Ãµes temporais (hora Ã— dia da semana).
4. ğŸŒ Integrar modelo a pipelines de previsÃ£o em tempo real (API/streaming).

---

**Autor:** Alexandre Marques Tortoza Canoa
**Data:** Outubro de 2025
**Projeto:** _Pesquisa Aplicada â€” PrevisÃ£o de Congestionamentos em SÃ£o Paulo_
